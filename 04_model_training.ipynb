{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T06:19:47.151466Z",
     "iopub.status.busy": "2023-11-28T06:19:47.150910Z",
     "iopub.status.idle": "2023-11-28T06:19:47.154206Z",
     "shell.execute_reply": "2023-11-28T06:19:47.153703Z",
     "shell.execute_reply.started": "2023-11-28T06:19:47.151448Z"
    },
    "id": "hqhXqcFw4X-X",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "#  1. [DONE ???] I switched from overfitting to bias, now is bias, I need more weights until the range returns to -180, 180\n",
    "#.      - add the tanh again at the en\n",
    "#.      - check the distributions of the scores, are there less negsatives? => balance the dataset by sampling\n",
    "#.      - is the mean of the true scores shifted, try stsandardizing the inputs and undoing it with the outputs. This is done AFTER the scaling to [-1. 1]\n",
    "#  2. [DONE] normalize inputs and multiply the outputs, remove the lambda layer\n",
    "#  3. [DONE] keras dtype policy 16 bit\n",
    "#  4. can we train an encoder-decoder in autoencoder setup to get the embeddings? Would that have the same dimensionality problem?\n",
    "#  5. [DONE] add the chemical signature of the compounds and chem features\n",
    "#  6. CV for evaluation and fix the rmrmse performance\n",
    "#  7. introduce gene features (biology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T06:19:47.902791Z",
     "iopub.status.busy": "2023-11-28T06:19:47.902293Z",
     "iopub.status.idle": "2023-11-28T06:19:48.762063Z",
     "shell.execute_reply": "2023-11-28T06:19:48.761386Z",
     "shell.execute_reply.started": "2023-11-28T06:19:47.902772Z"
    },
    "id": "JfJAhofmFq73",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/workspace\n",
      "total 5354032\n",
      "-rw-rw-r-- 1 jovyan jovyan  100924456 Nov  9 00:05 adata_excluded_ids.csv\n",
      "-rw-rw-r-- 1 jovyan jovyan   37227289 Sep 13 14:53 adata_obs_meta.csv\n",
      "-rw-rw-r-- 1 jovyan jovyan 1761529901 Sep 13 14:53 adata_train.parquet\n",
      "-rw-rw-r-- 1 jovyan jovyan  450105344 Oct 27 18:38 adata_train.parquet.F249d036\n",
      "-rw-rw-r-- 1 jovyan jovyan   92135550 Sep 13 14:53 de_train.parquet\n",
      "-rw-rw-r-- 1 jovyan jovyan       6723 Sep 13 14:53 id_map.csv\n",
      "-rw-rw-r-- 1 jovyan jovyan     943757 Sep 13 14:53 multiome_obs_meta.csv\n",
      "-rw-rw-r-- 1 jovyan jovyan 2555628667 Sep 13 14:53 multiome_train.parquet\n",
      "-rw-rw-r-- 1 jovyan jovyan  461373440 Oct 27 18:38 multiome_train.parquet.CD29e8F0\n",
      "-rw-rw-r-- 1 jovyan jovyan   13075170 Sep 13 14:53 multiome_var_meta.csv\n",
      "-rw-rw-r-- 1 jovyan jovyan   18711844 Sep 13 14:53 sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "! cd ~\n",
    "! pwd\n",
    "! mkdir -p scp/model\n",
    "# ! cp /kaggle/input/open-problems-single-cell-perturbations/* /home/jovyan/kaggle/working/scp/input\n",
    "# ! ls -l /content/drive/MyDrive/as/study/biotec/kaggle/scp/input\n",
    "! ls -l ../kaggle/input/open-problems-single-cell-perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-11-28T06:19:49.449959Z",
     "iopub.status.busy": "2023-11-28T06:19:49.449287Z",
     "iopub.status.idle": "2023-11-28T06:19:49.452448Z",
     "shell.execute_reply": "2023-11-28T06:19:49.451901Z",
     "shell.execute_reply.started": "2023-11-28T06:19:49.449938Z"
    },
    "id": "63SPqdHzM_uO",
    "outputId": "a4e520e1-03dd-4d2a-d5e8-ed273bf21acc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T06:19:50.003824Z",
     "iopub.status.busy": "2023-11-28T06:19:50.003603Z",
     "iopub.status.idle": "2023-11-28T06:19:50.006849Z",
     "shell.execute_reply": "2023-11-28T06:19:50.006281Z",
     "shell.execute_reply.started": "2023-11-28T06:19:50.003808Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! kaggle competitions download -c open-problems-single-cell-perturbations\n",
    "# ! unzip open-problems-single-cell-perturbations.zip -d /home/jovyan/kaggle/working/scp/input\n",
    "# ! rm open-problems-single-cell-perturbations.zip\n",
    "# ! ls -l /home/jovyan/kaggle/working/scp/input/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mS30bk6Xwe1"
   },
   "source": [
    "# Library installations and impots\n",
    "* * *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T06:19:50.930998Z",
     "iopub.status.busy": "2023-11-28T06:19:50.930778Z",
     "iopub.status.idle": "2023-11-28T06:19:50.933807Z",
     "shell.execute_reply": "2023-11-28T06:19:50.933242Z",
     "shell.execute_reply.started": "2023-11-28T06:19:50.930982Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install numpy==1.23.5\n",
    "# !pip uninstall tensorflow -y\n",
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T06:19:51.347711Z",
     "iopub.status.busy": "2023-11-28T06:19:51.347266Z",
     "iopub.status.idle": "2023-11-28T06:19:56.243766Z",
     "shell.execute_reply": "2023-11-28T06:19:56.243124Z",
     "shell.execute_reply.started": "2023-11-28T06:19:51.347693Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import json\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.activations import tanh\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "# set the dtype policy\n",
    "# mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T06:19:56.245346Z",
     "iopub.status.busy": "2023-11-28T06:19:56.244769Z",
     "iopub.status.idle": "2023-11-28T06:19:56.248332Z",
     "shell.execute_reply": "2023-11-28T06:19:56.247779Z",
     "shell.execute_reply.started": "2023-11-28T06:19:56.245327Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the seed using keras.utils.set_random_seed. This will set:\n",
    "# 1) `numpy` seed\n",
    "# 2) `tensorflow` random seed\n",
    "# 3) `python` random seed\n",
    "tf.keras.utils.set_random_seed(17)\n",
    "\n",
    "# This will make TensorFlow ops as deterministic as possible, but it will\n",
    "# affect the overall performance, so it's not enabled by default.\n",
    "# `enable_op_determinism()` is introduced in TensorFlow 2.9.\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T06:19:56.249111Z",
     "iopub.status.busy": "2023-11-28T06:19:56.248950Z",
     "iopub.status.idle": "2023-11-28T06:19:56.660185Z",
     "shell.execute_reply": "2023-11-28T06:19:56.659503Z",
     "shell.execute_reply.started": "2023-11-28T06:19:56.249097Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0', '/device:GPU:0']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 06:19:56.273127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-28 06:19:56.277273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-28 06:19:56.277532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-28 06:19:56.278284: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-28 06:19:56.278920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-28 06:19:56.279172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-28 06:19:56.279397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-28 06:19:56.656235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-28 06:19:56.656468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-28 06:19:56.656730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-28 06:19:56.656909: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /device:GPU:0 with 20779 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# check cuda\n",
    "tf.test.is_built_with_cuda(), tf.config.list_physical_devices('GPU')\n",
    "\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "print(get_available_devices())\n",
    "\n",
    "# Your output is probably something like ['/device:CPU:0']\n",
    "# It should be ['/device:CPU:0', '/device:GPU:0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T06:19:56.661480Z",
     "iopub.status.busy": "2023-11-28T06:19:56.661320Z",
     "iopub.status.idle": "2023-11-28T06:19:56.666494Z",
     "shell.execute_reply": "2023-11-28T06:19:56.665923Z",
     "shell.execute_reply.started": "2023-11-28T06:19:56.661465Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# constants\n",
    "# train_path = '/content/drive/MyDrive/as/study/biotec/kaggle/scp'\n",
    "# model_path = '/content/drive/MyDrive/as/study/biotec/kaggle/scp/model'\n",
    "data_path = '../kaggle/input/open-problems-single-cell-perturbations'\n",
    "model_path = './scp/model'\n",
    "intermediate_path = './scp/intermediate'\n",
    "\n",
    "OUTPUT_SCALE = 180\n",
    "\n",
    "EPOCHS = 500\n",
    "BATCH_SIZE = 1024\n",
    "PATIENCE = 100\n",
    "LR_PLATEAU_PATIENCE = 20\n",
    "LR_PLATEAU_FACTOR = 0.1\n",
    "VALIDATION_SPLIT = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDhkWstzYbxY"
   },
   "source": [
    "# Read training data\n",
    "* * *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "execution": {
     "iopub.execute_input": "2023-11-28T06:19:56.667253Z",
     "iopub.status.busy": "2023-11-28T06:19:56.667102Z",
     "iopub.status.idle": "2023-11-28T06:19:58.914627Z",
     "shell.execute_reply": "2023-11-28T06:19:58.914040Z",
     "shell.execute_reply.started": "2023-11-28T06:19:56.667240Z"
    },
    "id": "MtpUz4mIt81H",
    "outputId": "345ad224-57d4-4e22-c9d5-c80ce1cb0493",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 614 entries, 0 to 613\n",
      "Columns: 18216 entries, cell_type to ZZEF1\n",
      "dtypes: bool(1), float64(18211), object(4)\n",
      "memory usage: 85.3+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_parquet(f\"{data_path}/de_train.parquet\")\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T06:19:58.916001Z",
     "iopub.status.busy": "2023-11-28T06:19:58.915722Z",
     "iopub.status.idle": "2023-11-28T06:19:58.932372Z",
     "shell.execute_reply": "2023-11-28T06:19:58.931659Z",
     "shell.execute_reply.started": "2023-11-28T06:19:58.915985Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create name-id mappings for embeddings\n",
    "name_to_id = lambda names: {name:id for id, name in enumerate(sorted(set(names)))}\n",
    "name_to_id_maps = {\n",
    "    'cell_type': name_to_id(train_df['cell_type'].values),\n",
    "    'sm_name': name_to_id(train_df['sm_name'].values),\n",
    "}\n",
    "gene_names = list(train_df.columns[5:])\n",
    "name_to_id_maps['gene_name'] = name_to_id(gene_names)\n",
    "\n",
    "# save for use in prediction\n",
    "with open(f'{intermediate_path}/name_to_id_maps_for_embeddings.txt', 'w') as file:\n",
    "     file.write(json.dumps(name_to_id_maps)) # use `json.loads` to do the reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "execution": {
     "iopub.execute_input": "2023-11-28T06:19:58.933533Z",
     "iopub.status.busy": "2023-11-28T06:19:58.933225Z",
     "iopub.status.idle": "2023-11-28T06:19:59.006377Z",
     "shell.execute_reply": "2023-11-28T06:19:59.005763Z",
     "shell.execute_reply.started": "2023-11-28T06:19:58.933516Z"
    },
    "id": "zTVkj28sQJ5t",
    "outputId": "059b182f-034d-48d4-ff58-fd9e0994eeff",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(614, 18211)\n",
      "(614, 18211)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>A1BG</th>\n",
       "      <th>A1BG-AS1</th>\n",
       "      <th>A2M</th>\n",
       "      <th>A2M-AS1</th>\n",
       "      <th>A2MP1</th>\n",
       "      <th>A4GALT</th>\n",
       "      <th>AAAS</th>\n",
       "      <th>AACS</th>\n",
       "      <th>AAGAB</th>\n",
       "      <th>AAK1</th>\n",
       "      <th>...</th>\n",
       "      <th>ZUP1</th>\n",
       "      <th>ZW10</th>\n",
       "      <th>ZWILCH</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZXDA</th>\n",
       "      <th>ZXDB</th>\n",
       "      <th>ZXDC</th>\n",
       "      <th>ZYG11B</th>\n",
       "      <th>ZYX</th>\n",
       "      <th>ZZEF1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cell_type</th>\n",
       "      <th>sm_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NK cells</th>\n",
       "      <th>Clotrimazole</th>\n",
       "      <td>0.104720</td>\n",
       "      <td>-0.077524</td>\n",
       "      <td>-1.625596</td>\n",
       "      <td>-0.144545</td>\n",
       "      <td>0.143555</td>\n",
       "      <td>0.073229</td>\n",
       "      <td>-0.016823</td>\n",
       "      <td>0.101717</td>\n",
       "      <td>-0.005153</td>\n",
       "      <td>1.043629</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227781</td>\n",
       "      <td>-0.010752</td>\n",
       "      <td>-0.023881</td>\n",
       "      <td>0.674536</td>\n",
       "      <td>-0.453068</td>\n",
       "      <td>0.005164</td>\n",
       "      <td>-0.094959</td>\n",
       "      <td>0.034127</td>\n",
       "      <td>0.221377</td>\n",
       "      <td>0.368755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T cells CD4+</th>\n",
       "      <th>Clotrimazole</th>\n",
       "      <td>0.915953</td>\n",
       "      <td>-0.884380</td>\n",
       "      <td>0.371834</td>\n",
       "      <td>-0.081677</td>\n",
       "      <td>-0.498266</td>\n",
       "      <td>0.203559</td>\n",
       "      <td>0.604656</td>\n",
       "      <td>0.498592</td>\n",
       "      <td>-0.317184</td>\n",
       "      <td>0.375550</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.494985</td>\n",
       "      <td>-0.303419</td>\n",
       "      <td>0.304955</td>\n",
       "      <td>-0.333905</td>\n",
       "      <td>-0.315516</td>\n",
       "      <td>-0.369626</td>\n",
       "      <td>-0.095079</td>\n",
       "      <td>0.704780</td>\n",
       "      <td>1.096702</td>\n",
       "      <td>-0.869887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T cells CD8+</th>\n",
       "      <th>Clotrimazole</th>\n",
       "      <td>-0.387721</td>\n",
       "      <td>-0.305378</td>\n",
       "      <td>0.567777</td>\n",
       "      <td>0.303895</td>\n",
       "      <td>-0.022653</td>\n",
       "      <td>-0.480681</td>\n",
       "      <td>0.467144</td>\n",
       "      <td>-0.293205</td>\n",
       "      <td>-0.005098</td>\n",
       "      <td>0.214918</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.119422</td>\n",
       "      <td>-0.033608</td>\n",
       "      <td>-0.153123</td>\n",
       "      <td>0.183597</td>\n",
       "      <td>-0.555678</td>\n",
       "      <td>-1.494789</td>\n",
       "      <td>-0.213550</td>\n",
       "      <td>0.415768</td>\n",
       "      <td>0.078439</td>\n",
       "      <td>-0.259365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T regulatory cells</th>\n",
       "      <th>Clotrimazole</th>\n",
       "      <td>0.232893</td>\n",
       "      <td>0.129029</td>\n",
       "      <td>0.336897</td>\n",
       "      <td>0.486946</td>\n",
       "      <td>0.767661</td>\n",
       "      <td>0.718590</td>\n",
       "      <td>-0.162145</td>\n",
       "      <td>0.157206</td>\n",
       "      <td>-3.654218</td>\n",
       "      <td>-0.212402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.451679</td>\n",
       "      <td>0.704643</td>\n",
       "      <td>0.015468</td>\n",
       "      <td>-0.103868</td>\n",
       "      <td>0.865027</td>\n",
       "      <td>0.189114</td>\n",
       "      <td>0.224700</td>\n",
       "      <td>-0.048233</td>\n",
       "      <td>0.216139</td>\n",
       "      <td>-0.085024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NK cells</th>\n",
       "      <th>Mometasone Furoate</th>\n",
       "      <td>4.290652</td>\n",
       "      <td>-0.063864</td>\n",
       "      <td>-0.017443</td>\n",
       "      <td>-0.541154</td>\n",
       "      <td>0.570982</td>\n",
       "      <td>2.022829</td>\n",
       "      <td>0.600011</td>\n",
       "      <td>1.231275</td>\n",
       "      <td>0.236739</td>\n",
       "      <td>0.338703</td>\n",
       "      <td>...</td>\n",
       "      <td>0.758474</td>\n",
       "      <td>0.510762</td>\n",
       "      <td>0.607401</td>\n",
       "      <td>-0.123059</td>\n",
       "      <td>0.214366</td>\n",
       "      <td>0.487838</td>\n",
       "      <td>-0.819775</td>\n",
       "      <td>0.112365</td>\n",
       "      <td>-0.122193</td>\n",
       "      <td>0.676629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 18211 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           A1BG  A1BG-AS1       A2M   A2M-AS1  \\\n",
       "cell_type          sm_name                                                      \n",
       "NK cells           Clotrimazole        0.104720 -0.077524 -1.625596 -0.144545   \n",
       "T cells CD4+       Clotrimazole        0.915953 -0.884380  0.371834 -0.081677   \n",
       "T cells CD8+       Clotrimazole       -0.387721 -0.305378  0.567777  0.303895   \n",
       "T regulatory cells Clotrimazole        0.232893  0.129029  0.336897  0.486946   \n",
       "NK cells           Mometasone Furoate  4.290652 -0.063864 -0.017443 -0.541154   \n",
       "\n",
       "                                          A2MP1    A4GALT      AAAS      AACS  \\\n",
       "cell_type          sm_name                                                      \n",
       "NK cells           Clotrimazole        0.143555  0.073229 -0.016823  0.101717   \n",
       "T cells CD4+       Clotrimazole       -0.498266  0.203559  0.604656  0.498592   \n",
       "T cells CD8+       Clotrimazole       -0.022653 -0.480681  0.467144 -0.293205   \n",
       "T regulatory cells Clotrimazole        0.767661  0.718590 -0.162145  0.157206   \n",
       "NK cells           Mometasone Furoate  0.570982  2.022829  0.600011  1.231275   \n",
       "\n",
       "                                          AAGAB      AAK1  ...      ZUP1  \\\n",
       "cell_type          sm_name                                 ...             \n",
       "NK cells           Clotrimazole       -0.005153  1.043629  ... -0.227781   \n",
       "T cells CD4+       Clotrimazole       -0.317184  0.375550  ... -0.494985   \n",
       "T cells CD8+       Clotrimazole       -0.005098  0.214918  ... -0.119422   \n",
       "T regulatory cells Clotrimazole       -3.654218 -0.212402  ...  0.451679   \n",
       "NK cells           Mometasone Furoate  0.236739  0.338703  ...  0.758474   \n",
       "\n",
       "                                           ZW10    ZWILCH     ZWINT      ZXDA  \\\n",
       "cell_type          sm_name                                                      \n",
       "NK cells           Clotrimazole       -0.010752 -0.023881  0.674536 -0.453068   \n",
       "T cells CD4+       Clotrimazole       -0.303419  0.304955 -0.333905 -0.315516   \n",
       "T cells CD8+       Clotrimazole       -0.033608 -0.153123  0.183597 -0.555678   \n",
       "T regulatory cells Clotrimazole        0.704643  0.015468 -0.103868  0.865027   \n",
       "NK cells           Mometasone Furoate  0.510762  0.607401 -0.123059  0.214366   \n",
       "\n",
       "                                           ZXDB      ZXDC    ZYG11B       ZYX  \\\n",
       "cell_type          sm_name                                                      \n",
       "NK cells           Clotrimazole        0.005164 -0.094959  0.034127  0.221377   \n",
       "T cells CD4+       Clotrimazole       -0.369626 -0.095079  0.704780  1.096702   \n",
       "T cells CD8+       Clotrimazole       -1.494789 -0.213550  0.415768  0.078439   \n",
       "T regulatory cells Clotrimazole        0.189114  0.224700 -0.048233  0.216139   \n",
       "NK cells           Mometasone Furoate  0.487838 -0.819775  0.112365 -0.122193   \n",
       "\n",
       "                                          ZZEF1  \n",
       "cell_type          sm_name                       \n",
       "NK cells           Clotrimazole        0.368755  \n",
       "T cells CD4+       Clotrimazole       -0.869887  \n",
       "T cells CD8+       Clotrimazole       -0.259365  \n",
       "T regulatory cells Clotrimazole       -0.085024  \n",
       "NK cells           Mometasone Furoate  0.676629  \n",
       "\n",
       "[5 rows x 18211 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape train_df\n",
    "_ = train_df.set_index(['cell_type', 'sm_name'], inplace=True)\n",
    "train_df.drop(['sm_lincs_id', 'SMILES', 'control'], inplace=True, axis=1)\n",
    "print(train_df.shape)\n",
    "df_train = train_df.dropna()\n",
    "print(train_df.shape)\n",
    "train_df.columns[:5]\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-11-28T06:19:59.007494Z",
     "iopub.status.busy": "2023-11-28T06:19:59.007201Z",
     "iopub.status.idle": "2023-11-28T06:19:59.030092Z",
     "shell.execute_reply": "2023-11-28T06:19:59.029470Z",
     "shell.execute_reply.started": "2023-11-28T06:19:59.007477Z"
    },
    "id": "zy50yr8vr7tw",
    "outputId": "7849766f-bd8e-4a7c-e6da-7a52585e4f69",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-180.5192016034818 179.32417689610105\n",
      "180\n"
     ]
    }
   ],
   "source": [
    "print(train_df.values.min(), train_df.values.max())\n",
    "OUTPUT_SCALE = int(round((train_df.values.max() - train_df.values.min()))/2)\n",
    "print(OUTPUT_SCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-11-28T06:19:59.031300Z",
     "iopub.status.busy": "2023-11-28T06:19:59.030942Z",
     "iopub.status.idle": "2023-11-28T06:19:59.035358Z",
     "shell.execute_reply": "2023-11-28T06:19:59.034773Z",
     "shell.execute_reply.started": "2023-11-28T06:19:59.031283Z"
    },
    "id": "uEZFMmlSiW6z",
    "outputId": "33ea6535-6276-401c-80c3-3e37f1108fa5",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,\n",
       " {'B cells': 0,\n",
       "  'Myeloid cells': 1,\n",
       "  'NK cells': 2,\n",
       "  'T cells CD4+': 3,\n",
       "  'T cells CD8+': 4,\n",
       "  'T regulatory cells': 5})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_to_id_maps['cell_type']['NK cells'], name_to_id_maps['cell_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read features\n",
    "* * *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T06:44:20.700882Z",
     "iopub.status.busy": "2023-11-28T06:44:20.700511Z",
     "iopub.status.idle": "2023-11-28T06:44:20.720399Z",
     "shell.execute_reply": "2023-11-28T06:44:20.719794Z",
     "shell.execute_reply.started": "2023-11-28T06:44:20.700865Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 146 small molecule fingerprints from path:\n",
      "./scp/intermediate/fingerprints.csv\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sm_name</th>\n",
       "      <th>fp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Clotrimazole</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mometasone Furoate</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Idelalisib</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vandetanib</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bosutinib</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>CGM-097</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>TGX 221</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Azacitidine</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Atorvastatin</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Riociguat</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>146 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                sm_name                                                 fp\n",
       "0          Clotrimazole  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "1    Mometasone Furoate  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "2            Idelalisib  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "3            Vandetanib  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "4             Bosutinib  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "..                  ...                                                ...\n",
       "141             CGM-097  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "142             TGX 221  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "143         Azacitidine  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "144        Atorvastatin  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "145           Riociguat  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "\n",
       "[146 rows x 2 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp_path = f\"{intermediate_path}/fingerprints.csv\"\n",
    "fp_df = pd.read_csv(fp_path)\n",
    "print(f'Read {fp_df.shape[0]} small molecule fingerprints from path:\\n{fp_path}')\n",
    "fp_df['fp'] = fp_df['fingerprint'].apply(lambda x: list(map(int, x[1:-1].split(','))))\n",
    "fp_df.drop(columns=['fingerprint'], inplace=True)\n",
    "name_to_fp = {row[0]: row[1] for row in fp_df.values}\n",
    "print(name_to_fp['Clotrimazole'])\n",
    "fp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T06:44:30.396495Z",
     "iopub.status.busy": "2023-11-28T06:44:30.396130Z",
     "iopub.status.idle": "2023-11-28T06:44:30.408090Z",
     "shell.execute_reply": "2023-11-28T06:44:30.407516Z",
     "shell.execute_reply.started": "2023-11-28T06:44:30.396477Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 146 small molecule descriptors from path:\n",
      "./scp/intermediate/ro5_descriptors.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sm_name</th>\n",
       "      <th>MW</th>\n",
       "      <th>HBA</th>\n",
       "      <th>HBD</th>\n",
       "      <th>LogP</th>\n",
       "      <th>pass_ro5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Clotrimazole</td>\n",
       "      <td>344.845</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5.37670</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mometasone Furoate</td>\n",
       "      <td>521.437</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4.86920</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Idelalisib</td>\n",
       "      <td>415.432</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3.75430</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vandetanib</td>\n",
       "      <td>475.362</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5.00420</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bosutinib</td>\n",
       "      <td>530.456</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>5.19038</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>CGM-097</td>\n",
       "      <td>659.271</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>6.58350</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>TGX 221</td>\n",
       "      <td>364.449</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3.01262</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Azacitidine</td>\n",
       "      <td>244.207</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>-3.16800</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Atorvastatin</td>\n",
       "      <td>558.650</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>6.31360</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Riociguat</td>\n",
       "      <td>422.424</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2.44270</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>146 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                sm_name       MW  HBA  HBD     LogP  pass_ro5\n",
       "0          Clotrimazole  344.845    2    0  5.37670      True\n",
       "1    Mometasone Furoate  521.437    6    1  4.86920      True\n",
       "2            Idelalisib  415.432    8    2  3.75430      True\n",
       "3            Vandetanib  475.362    6    1  5.00420      True\n",
       "4             Bosutinib  530.456    8    1  5.19038     False\n",
       "..                  ...      ...  ...  ...      ...       ...\n",
       "141             CGM-097  659.271    8    0  6.58350     False\n",
       "142             TGX 221  364.449    6    1  3.01262      True\n",
       "143         Azacitidine  244.207    9    5 -3.16800      True\n",
       "144        Atorvastatin  558.650    7    4  6.31360     False\n",
       "145           Riociguat  422.424   10    4  2.44270      True\n",
       "\n",
       "[146 rows x 6 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ro5_path = f\"{intermediate_path}/ro5_descriptors.csv\"\n",
    "ro5_df = pd.read_csv(ro5_path)\n",
    "print(f'Read {ro5_df.shape[0]} small molecule descriptors from path:\\n{ro5_path}')\n",
    "ro5_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T06:44:30.745362Z",
     "iopub.status.busy": "2023-11-28T06:44:30.744857Z",
     "iopub.status.idle": "2023-11-28T06:44:30.750706Z",
     "shell.execute_reply": "2023-11-28T06:44:30.750154Z",
     "shell.execute_reply.started": "2023-11-28T06:44:30.745345Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([344.845, 2, 0, 5.3767, 1], dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# name_to_ro5 = {sm_name: ro5_df[['MW', 'HBA', 'HBD', 'LogP', 'pass_ro5']]}\n",
    "ro5_df['pass_ro5'] = ro5_df['pass_ro5'].astype('int')\n",
    "ro5_df['LogP'] = ro5_df['LogP'].round(4)\n",
    "name_to_ro5 = {row[0]: row[1:] for row in ro5_df.values}\n",
    "name_to_ro5['Clotrimazole']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FaLqnmAYaV4p"
   },
   "source": [
    "# Modeling considerations\n",
    "* * *\n",
    "## Input\n",
    "- cell_type: integer (5 categories)\n",
    "- sm_name: integer (144 categories)\n",
    "- gene_name: integer (18,000 categories)\n",
    "- ro5: array of shape=[5], float. Chemical features of the sm.\n",
    "- fp: fingerprints (maccs keys). 167 binary flags. \n",
    "\n",
    "## Ignored inputs\n",
    "- control: this is a dependent variable, it can be inferred from sm_name.  \n",
    "- SMILES:  temporarily not used, we may add qhemical features at a later stage.  \n",
    "  \n",
    "## Output\n",
    "- differential gene expression: real number\n",
    "  \n",
    "## Representation:\n",
    "- We need to encode some of the inputs with dense vectors, given the cardinality of the gene_name input, we can't use sparse one-hot vectors (aka dummy variables) because it would take too much memory.\n",
    "  \n",
    "## Deep Learning Model\n",
    "- The categorical inputs are mapped to integer ids. Then, each id is, in tern, mapped to a short dense vector, which is initialized randomly and trained with the gradient backpropagation method.  The training gradually changes these vectors generating numeric differences between them that mirror the differences between the categorical inputs that they represent. This is known in deep learning as an \"embedding layer\".  \n",
    "- The output is a real number between -30 and 30, and the exploratory data analysis has shown that the distribution of the outputs is approximately normal with mean 0, therefore the output is modeled with a hyperbolic tangent function, which outputs a range [-1, 1], multiplied by a constant of value 30.    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhYxAe05qLXB"
   },
   "source": [
    "# Create the feature matrix for training\n",
    "* * *\n",
    "The feature matrix is a mapping with a tuple feature ids as key and the true label as value.\n",
    "This mapping has two uses, first it in training and later for computing the rmse of the predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-11-28T06:44:42.675393Z",
     "iopub.status.busy": "2023-11-28T06:44:42.674932Z",
     "iopub.status.idle": "2023-11-28T06:45:26.057245Z",
     "shell.execute_reply": "2023-11-28T06:45:26.056554Z",
     "shell.execute_reply.started": "2023-11-28T06:44:42.675376Z"
    },
    "id": "U2Ln1Ez8-Qsu",
    "outputId": "7d2ca918-cf1f-40d7-a7f9-57463e048b85",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell_types.shape: 6\n",
      "sm_names.shape: 146\n"
     ]
    }
   ],
   "source": [
    "feature_matrix_train = OrderedDict()\n",
    "\n",
    "# get columns as lists for code clarity\n",
    "cell_types, sm_names = zip(*train_df.index.tolist())\n",
    "de_values = train_df[gene_names].values\n",
    "\n",
    "#\n",
    "for cell_type, sm_name, diff_expressions in zip(cell_types, sm_names, de_values):\n",
    "  cell_type_id = name_to_id_maps['cell_type'][cell_type]\n",
    "  sm_name_id = name_to_id_maps['sm_name'][sm_name]\n",
    "  ro5 = name_to_ro5[sm_name]\n",
    "  fp = name_to_fp[sm_name]\n",
    "\n",
    "  # gene differential expression columns\n",
    "  for gene_name, diff_expression in zip(gene_names, diff_expressions):\n",
    "    gene_name_id = name_to_id_maps['gene_name'][gene_name]\n",
    "    feature_matrix_train[(cell_type_id, sm_name_id, gene_name_id, *ro5, *fp)] = diff_expression\n",
    "\n",
    "print(f'cell_types.shape: {len(set(cell_types))}')\n",
    "print(f'sm_names.shape: {len(set(sm_names))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T06:46:09.379857Z",
     "iopub.status.busy": "2023-11-28T06:46:09.379453Z",
     "iopub.status.idle": "2023-11-28T06:46:09.382501Z",
     "shell.execute_reply": "2023-11-28T06:46:09.381954Z",
     "shell.execute_reply.started": "2023-11-28T06:46:09.379840Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list(feature_matrix_train.keys())[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvZ-8d5_qfc_"
   },
   "source": [
    "# Define the neural network\n",
    "* * *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-11-28T06:48:34.052082Z",
     "iopub.status.busy": "2023-11-28T06:48:34.051836Z",
     "iopub.status.idle": "2023-11-28T06:48:34.229139Z",
     "shell.execute_reply": "2023-11-28T06:48:34.228174Z",
     "shell.execute_reply.started": "2023-11-28T06:48:34.052063Z"
    },
    "id": "XfsjA-Ko_6Oj",
    "outputId": "b7b6455c-4c4b-45a0-f3a3-b9c962bd48ee",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_7 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 1, 32)        192         ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)        (None, 1, 8)         1168        ['input_7[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_5 (Embedding)        (None, 1, 512)       9324032     ['input_8[0][0]']                \n",
      "                                                                                                  \n",
      " input_9 (InputLayer)           [(None, 1, 5)]       0           []                               \n",
      "                                                                                                  \n",
      " input_10 (InputLayer)          [(None, 1, 167)]     0           []                               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 1, 724)       0           ['embedding_3[0][0]',            \n",
      "                                                                  'embedding_4[0][0]',            \n",
      "                                                                  'embedding_5[0][0]',            \n",
      "                                                                  'input_9[0][0]',                \n",
      "                                                                  'input_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 1, 256)       185600      ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 1, 256)      1024        ['dense_6[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, 1, 256)       0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 1, 256)       0           ['leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 1, 128)       32896       ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 1, 128)      512         ['dense_7[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_6 (LeakyReLU)      (None, 1, 128)       0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 1, 128)       0           ['leaky_re_lu_6[0][0]']          \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 1, 64)        8256        ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 1, 64)       256         ['dense_8[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_7 (LeakyReLU)      (None, 1, 64)        0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1, 64)        4160        ['leaky_re_lu_7[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 1, 64)       256         ['dense_9[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_8 (LeakyReLU)      (None, 1, 64)        0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 1, 32)        2080        ['leaky_re_lu_8[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 1, 32)       128         ['dense_10[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_9 (LeakyReLU)      (None, 1, 32)        0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 1, 1)         33          ['leaky_re_lu_9[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,560,593\n",
      "Trainable params: 9,559,505\n",
      "Non-trainable params: 1,088\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define the NN\n",
    "# feed forward regressor with a trainable\n",
    "# embedding layer that accepts 4 integers,\n",
    "# and a single-unit output layer\n",
    "\n",
    "input_cell = Input(shape=(1,), dtype='int16')\n",
    "input_drug = Input(shape=(1,), dtype='int16')\n",
    "input_gene = Input(shape=(1,), dtype='int16')\n",
    "input_drug_ro5 = Input(shape=(1,5,), dtype='float32')\n",
    "input_drug_fp = Input(shape=(1,167,), dtype='float32')\n",
    "\n",
    "embedding_cell = Embedding(input_dim=len(name_to_id_maps['cell_type']), output_dim=32,\n",
    "                        embeddings_regularizer=tf.keras.regularizers.L2(0.01))(input_cell)\n",
    "\n",
    "embedding_drug = Embedding(input_dim=len(name_to_id_maps['sm_name']), output_dim=8,\n",
    "                        embeddings_regularizer=tf.keras.regularizers.L2(0.01))(input_drug)\n",
    "\n",
    "embedding_gene = Embedding(input_dim=len(name_to_id_maps['gene_name']), input_length=1, output_dim=512,\n",
    "                        embeddings_regularizer=tf.keras.regularizers.L2(0.01))(input_gene)\n",
    "\n",
    "concatenated = Concatenate()([embedding_cell, embedding_drug, embedding_gene, input_drug_ro5, input_drug_fp])\n",
    "\n",
    "# dense layers \n",
    "middle_layers = Dense(256)(concatenated)\n",
    "middle_layers = BatchNormalization()(middle_layers)\n",
    "middle_layers = LeakyReLU(.15)(middle_layers)\n",
    "middle_layers = Dropout(.2)(middle_layers)\n",
    "\n",
    "middle_layers = Dense(128)(middle_layers)\n",
    "middle_layers = BatchNormalization()(middle_layers)\n",
    "middle_layers = LeakyReLU(.15)(middle_layers)\n",
    "middle_layers = Dropout(.2)(middle_layers)\n",
    "\n",
    "middle_layers = Dense(64)(middle_layers)\n",
    "middle_layers = BatchNormalization()(middle_layers)\n",
    "middle_layers = LeakyReLU(.15)(middle_layers)\n",
    "# middle_layers = Dropout(.2)(middle_layers)\n",
    "\n",
    "middle_layers = Dense(64)(middle_layers)\n",
    "middle_layers = BatchNormalization()(middle_layers)\n",
    "middle_layers = LeakyReLU(.15)(middle_layers)\n",
    "# middle_layers = Dropout(.2)(middle_layers)\n",
    "\n",
    "middle_layers = Dense(32)(middle_layers)\n",
    "middle_layers = BatchNormalization()(middle_layers)\n",
    "middle_layers = LeakyReLU(.15)(middle_layers)\n",
    "\n",
    "# output\n",
    "output = Dense(1)(middle_layers)\n",
    "# output = tanh(output)\n",
    "# output = Lambda(lambda x: x * OUTPUT_SCALE)(output)\n",
    "\n",
    "regressor = Model(inputs=[input_cell, input_drug, input_gene, input_drug_ro5, input_drug_fp], outputs=[output])\n",
    "\n",
    "# Compile the regressor\n",
    "regressor.compile(loss='mse', optimizer='adam', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "regressor.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XymlzbDhqjLb"
   },
   "source": [
    "# Training\n",
    "* * *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T06:48:52.821917Z",
     "iopub.status.busy": "2023-11-28T06:48:52.821514Z",
     "iopub.status.idle": "2023-11-28T06:48:52.824764Z",
     "shell.execute_reply": "2023-11-28T06:48:52.824238Z",
     "shell.execute_reply.started": "2023-11-28T06:48:52.821900Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! rm -rf .ipynb_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T06:48:53.435143Z",
     "iopub.status.busy": "2023-11-28T06:48:53.434747Z",
     "iopub.status.idle": "2023-11-28T06:48:53.857586Z",
     "shell.execute_reply": "2023-11-28T06:48:53.856931Z",
     "shell.execute_reply.started": "2023-11-28T06:48:53.435126Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! rm -rf .mdl_wts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T06:48:53.952349Z",
     "iopub.status.busy": "2023-11-28T06:48:53.951816Z",
     "iopub.status.idle": "2023-11-28T06:48:53.955318Z",
     "shell.execute_reply": "2023-11-28T06:48:53.954859Z",
     "shell.execute_reply.started": "2023-11-28T06:48:53.952330Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_filepath = '.mdl_wts/'\n",
    "if os.path.exists(checkpoint_filepath):\n",
    "    print(f'Loading model weights from: {checkpoint_filepath}')\n",
    "    regressor.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T06:48:54.626583Z",
     "iopub.status.busy": "2023-11-28T06:48:54.626039Z",
     "iopub.status.idle": "2023-11-28T06:48:55.430162Z",
     "shell.execute_reply": "2023-11-28T06:48:55.429587Z",
     "shell.execute_reply.started": "2023-11-28T06:48:54.626564Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 560\n",
      "drwxrwsr-x  6 jovyan jovyan   4096 Nov 28 06:48 .\n",
      "drwxr-Sr-- 19 jovyan jovyan   4096 Oct 27 23:43 ..\n",
      "-rw-rw-r--  1 jovyan jovyan 239497 Oct 29 23:00 03_model_training.ipynb\n",
      "-rw-rw-r--  1 jovyan jovyan 230567 Nov 28 06:48 04_model_training.ipynb\n",
      "-rw-rw-r--  1 jovyan jovyan  64330 Oct 31 14:53 05_prediction.ipynb\n",
      "-rw-rw-r--  1 jovyan jovyan     73 Oct 27 23:25 checkpoint\n",
      "drwxrwsr-x  2 jovyan jovyan   4096 Oct 30 02:38 .ipynb_checkpoints\n",
      "drwxr-sr-x  2 jovyan jovyan   4096 Nov 28 06:48 .mdl_wts\n",
      "-rw-rw-r--  1 jovyan jovyan   2501 Oct 30 02:37 name_to_id_maps_for_embeddings.txt\n",
      "drwxrwsr-x  3 jovyan jovyan   4096 Oct 27 16:37 neurips-2023-scripts\n",
      "drwxrwsr-x  6 jovyan jovyan   4096 Oct 28 23:13 scp\n"
     ]
    }
   ],
   "source": [
    "! mkdir .mdl_wts/\n",
    "! ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T06:52:38.294224Z",
     "iopub.status.busy": "2023-11-28T06:52:38.293786Z",
     "iopub.status.idle": "2023-11-28T06:52:51.556739Z",
     "shell.execute_reply": "2023-11-28T06:52:51.556253Z",
     "shell.execute_reply.started": "2023-11-28T06:52:38.294207Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.10472047446898534,\n",
       " -0.07752420930615231,\n",
       " -1.6255960356468964,\n",
       " -0.14454470896369243,\n",
       " 0.14355468030117133]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(feature_matrix_train.values())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-11-28T06:58:08.757321Z",
     "iopub.status.busy": "2023-11-28T06:58:08.756883Z",
     "iopub.status.idle": "2023-11-28T13:56:59.812306Z",
     "shell.execute_reply": "2023-11-28T13:56:59.811615Z",
     "shell.execute_reply.started": "2023-11-28T06:58:08.757303Z"
    },
    "id": "XuD_4VY167i-",
    "outputId": "c364b3b8-05c6-4aba-e880-d31e53b00183",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 07:01:34.863786: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 7394584984 exceeds 10% of free system memory.\n",
      "2023-11-28 07:01:38.075919: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 7394584984 exceeds 10% of free system memory.\n",
      "2023-11-28 07:01:40.622781: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 7394584984 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "   22/10811 [..............................] - ETA: 1:22 - loss: 48.2803 - root_mean_squared_error: 2.5049"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 07:01:46.977562: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10808/10811 [============================>.] - ETA: 0s - loss: 4.2417 - root_mean_squared_error: 1.8302\n",
      "Epoch 1: val_loss improved from inf to 1.17753, saving model to .mdl_wts/\n",
      "10811/10811 [==============================] - 87s 8ms/step - loss: 4.2413 - root_mean_squared_error: 1.8301 - val_loss: 1.1775 - val_root_mean_squared_error: 0.6431 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "10809/10811 [============================>.] - ETA: 0s - loss: 3.5310 - root_mean_squared_error: 1.6891\n",
      "Epoch 2: val_loss improved from 1.17753 to 1.04690, saving model to .mdl_wts/\n",
      "10811/10811 [==============================] - 86s 8ms/step - loss: 3.5310 - root_mean_squared_error: 1.6891 - val_loss: 1.0469 - val_root_mean_squared_error: 0.6245 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 3.3652 - root_mean_squared_error: 1.6500\n",
      "Epoch 3: val_loss did not improve from 1.04690\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 3.3651 - root_mean_squared_error: 1.6500 - val_loss: 1.0641 - val_root_mean_squared_error: 0.6512 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 3.2428 - root_mean_squared_error: 1.6231\n",
      "Epoch 4: val_loss did not improve from 1.04690\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 3.2429 - root_mean_squared_error: 1.6231 - val_loss: 1.1236 - val_root_mean_squared_error: 0.7085 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 3.1759 - root_mean_squared_error: 1.6080\n",
      "Epoch 5: val_loss did not improve from 1.04690\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 3.1759 - root_mean_squared_error: 1.6080 - val_loss: 1.0577 - val_root_mean_squared_error: 0.6878 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 3.1440 - root_mean_squared_error: 1.5995\n",
      "Epoch 6: val_loss did not improve from 1.04690\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 3.1448 - root_mean_squared_error: 1.5998 - val_loss: 1.1030 - val_root_mean_squared_error: 0.7150 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 3.1099 - root_mean_squared_error: 1.5893\n",
      "Epoch 7: val_loss improved from 1.04690 to 0.97538, saving model to .mdl_wts/\n",
      "10811/10811 [==============================] - 86s 8ms/step - loss: 3.1097 - root_mean_squared_error: 1.5892 - val_loss: 0.9754 - val_root_mean_squared_error: 0.6377 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 3.0383 - root_mean_squared_error: 1.5723\n",
      "Epoch 8: val_loss did not improve from 0.97538\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 3.0412 - root_mean_squared_error: 1.5732 - val_loss: 0.9935 - val_root_mean_squared_error: 0.6528 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 3.0074 - root_mean_squared_error: 1.5645\n",
      "Epoch 9: val_loss did not improve from 0.97538\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 3.0073 - root_mean_squared_error: 1.5645 - val_loss: 1.3195 - val_root_mean_squared_error: 0.8753 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 2.9906 - root_mean_squared_error: 1.5629\n",
      "Epoch 10: val_loss did not improve from 0.97538\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 2.9906 - root_mean_squared_error: 1.5629 - val_loss: 1.2858 - val_root_mean_squared_error: 0.8648 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 2.9734 - root_mean_squared_error: 1.5593\n",
      "Epoch 11: val_loss improved from 0.97538 to 0.97039, saving model to .mdl_wts/\n",
      "10811/10811 [==============================] - 86s 8ms/step - loss: 2.9739 - root_mean_squared_error: 1.5594 - val_loss: 0.9704 - val_root_mean_squared_error: 0.6441 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 2.9797 - root_mean_squared_error: 1.5584\n",
      "Epoch 12: val_loss did not improve from 0.97039\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 2.9797 - root_mean_squared_error: 1.5584 - val_loss: 1.1201 - val_root_mean_squared_error: 0.7799 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "10809/10811 [============================>.] - ETA: 0s - loss: 2.9711 - root_mean_squared_error: 1.5555\n",
      "Epoch 13: val_loss did not improve from 0.97039\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 2.9710 - root_mean_squared_error: 1.5555 - val_loss: 1.1827 - val_root_mean_squared_error: 0.8000 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 2.9922 - root_mean_squared_error: 1.5525\n",
      "Epoch 14: val_loss did not improve from 0.97039\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 2.9924 - root_mean_squared_error: 1.5525 - val_loss: 1.2508 - val_root_mean_squared_error: 0.7818 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 3.0137 - root_mean_squared_error: 1.5500\n",
      "Epoch 15: val_loss did not improve from 0.97039\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 3.0142 - root_mean_squared_error: 1.5502 - val_loss: 0.9784 - val_root_mean_squared_error: 0.6210 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "10809/10811 [============================>.] - ETA: 0s - loss: 3.0210 - root_mean_squared_error: 1.5443\n",
      "Epoch 16: val_loss did not improve from 0.97039\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 3.0209 - root_mean_squared_error: 1.5443 - val_loss: 1.5842 - val_root_mean_squared_error: 0.9659 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 3.0309 - root_mean_squared_error: 1.5423\n",
      "Epoch 17: val_loss did not improve from 0.97039\n",
      "10811/10811 [==============================] - 86s 8ms/step - loss: 3.0309 - root_mean_squared_error: 1.5423 - val_loss: 1.0236 - val_root_mean_squared_error: 0.6454 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 3.0177 - root_mean_squared_error: 1.5387\n",
      "Epoch 18: val_loss did not improve from 0.97039\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 3.0175 - root_mean_squared_error: 1.5387 - val_loss: 1.1003 - val_root_mean_squared_error: 0.6785 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 3.0187 - root_mean_squared_error: 1.5388\n",
      "Epoch 19: val_loss did not improve from 0.97039\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 3.0189 - root_mean_squared_error: 1.5389 - val_loss: 1.3347 - val_root_mean_squared_error: 0.8148 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 3.0019 - root_mean_squared_error: 1.5335\n",
      "Epoch 20: val_loss did not improve from 0.97039\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 3.0019 - root_mean_squared_error: 1.5335 - val_loss: 1.1259 - val_root_mean_squared_error: 0.6986 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 3.0242 - root_mean_squared_error: 1.5365\n",
      "Epoch 21: val_loss did not improve from 0.97039\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 3.0242 - root_mean_squared_error: 1.5365 - val_loss: 1.5331 - val_root_mean_squared_error: 0.9294 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 3.0067 - root_mean_squared_error: 1.5328\n",
      "Epoch 22: val_loss did not improve from 0.97039\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 3.0067 - root_mean_squared_error: 1.5328 - val_loss: 1.3175 - val_root_mean_squared_error: 0.7771 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "10809/10811 [============================>.] - ETA: 0s - loss: 3.0117 - root_mean_squared_error: 1.5319\n",
      "Epoch 23: val_loss did not improve from 0.97039\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 3.0116 - root_mean_squared_error: 1.5319 - val_loss: 1.1471 - val_root_mean_squared_error: 0.6759 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "10809/10811 [============================>.] - ETA: 0s - loss: 3.0105 - root_mean_squared_error: 1.5305\n",
      "Epoch 24: val_loss did not improve from 0.97039\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 3.0104 - root_mean_squared_error: 1.5305 - val_loss: 1.0785 - val_root_mean_squared_error: 0.6379 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "10809/10811 [============================>.] - ETA: 0s - loss: 3.0186 - root_mean_squared_error: 1.5326\n",
      "Epoch 25: val_loss did not improve from 0.97039\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 3.0185 - root_mean_squared_error: 1.5326 - val_loss: 4.2634 - val_root_mean_squared_error: 1.8790 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 3.0063 - root_mean_squared_error: 1.5277\n",
      "Epoch 26: val_loss did not improve from 0.97039\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 3.0066 - root_mean_squared_error: 1.5278 - val_loss: 1.0713 - val_root_mean_squared_error: 0.6470 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 3.0140 - root_mean_squared_error: 1.5300\n",
      "Epoch 27: val_loss did not improve from 0.97039\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 3.0139 - root_mean_squared_error: 1.5300 - val_loss: 1.1199 - val_root_mean_squared_error: 0.6984 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 2.9974 - root_mean_squared_error: 1.5247\n",
      "Epoch 28: val_loss did not improve from 0.97039\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 2.9974 - root_mean_squared_error: 1.5247 - val_loss: 1.3314 - val_root_mean_squared_error: 0.7886 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 3.0074 - root_mean_squared_error: 1.5269\n",
      "Epoch 29: val_loss did not improve from 0.97039\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 3.0074 - root_mean_squared_error: 1.5269 - val_loss: 1.2275 - val_root_mean_squared_error: 0.7488 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 3.0025 - root_mean_squared_error: 1.5263\n",
      "Epoch 30: val_loss did not improve from 0.97039\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 3.0024 - root_mean_squared_error: 1.5263 - val_loss: 1.2959 - val_root_mean_squared_error: 0.8090 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 2.9931 - root_mean_squared_error: 1.5230\n",
      "Epoch 31: val_loss did not improve from 0.97039\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 2.9929 - root_mean_squared_error: 1.5230 - val_loss: 1.1355 - val_root_mean_squared_error: 0.7147 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "10809/10811 [============================>.] - ETA: 0s - loss: 2.2030 - root_mean_squared_error: 1.3926\n",
      "Epoch 32: val_loss improved from 0.97039 to 0.71591, saving model to .mdl_wts/\n",
      "10811/10811 [==============================] - 86s 8ms/step - loss: 2.2030 - root_mean_squared_error: 1.3926 - val_loss: 0.7159 - val_root_mean_squared_error: 0.7205 - lr: 1.0000e-04\n",
      "Epoch 33/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.9624 - root_mean_squared_error: 1.3368\n",
      "Epoch 33: val_loss improved from 0.71591 to 0.65053, saving model to .mdl_wts/\n",
      "10811/10811 [==============================] - 86s 8ms/step - loss: 1.9623 - root_mean_squared_error: 1.3368 - val_loss: 0.6505 - val_root_mean_squared_error: 0.6993 - lr: 1.0000e-04\n",
      "Epoch 34/500\n",
      "10809/10811 [============================>.] - ETA: 0s - loss: 1.9029 - root_mean_squared_error: 1.3230\n",
      "Epoch 34: val_loss did not improve from 0.65053\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.9029 - root_mean_squared_error: 1.3230 - val_loss: 0.6688 - val_root_mean_squared_error: 0.7231 - lr: 1.0000e-04\n",
      "Epoch 35/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.8818 - root_mean_squared_error: 1.3185\n",
      "Epoch 35: val_loss did not improve from 0.65053\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.8818 - root_mean_squared_error: 1.3185 - val_loss: 0.7201 - val_root_mean_squared_error: 0.7607 - lr: 1.0000e-04\n",
      "Epoch 36/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.8568 - root_mean_squared_error: 1.3103\n",
      "Epoch 36: val_loss did not improve from 0.65053\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.8569 - root_mean_squared_error: 1.3103 - val_loss: 0.7868 - val_root_mean_squared_error: 0.8042 - lr: 1.0000e-04\n",
      "Epoch 37/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.8394 - root_mean_squared_error: 1.3034\n",
      "Epoch 37: val_loss did not improve from 0.65053\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.8392 - root_mean_squared_error: 1.3034 - val_loss: 0.7536 - val_root_mean_squared_error: 0.7823 - lr: 1.0000e-04\n",
      "Epoch 38/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.8263 - root_mean_squared_error: 1.2980\n",
      "Epoch 38: val_loss did not improve from 0.65053\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.8263 - root_mean_squared_error: 1.2980 - val_loss: 0.7493 - val_root_mean_squared_error: 0.7792 - lr: 1.0000e-04\n",
      "Epoch 39/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.8205 - root_mean_squared_error: 1.2946\n",
      "Epoch 39: val_loss did not improve from 0.65053\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.8205 - root_mean_squared_error: 1.2946 - val_loss: 0.6680 - val_root_mean_squared_error: 0.7235 - lr: 1.0000e-04\n",
      "Epoch 40/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.8123 - root_mean_squared_error: 1.2911\n",
      "Epoch 40: val_loss did not improve from 0.65053\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.8123 - root_mean_squared_error: 1.2911 - val_loss: 0.7623 - val_root_mean_squared_error: 0.7832 - lr: 1.0000e-04\n",
      "Epoch 41/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.8048 - root_mean_squared_error: 1.2874\n",
      "Epoch 41: val_loss did not improve from 0.65053\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.8047 - root_mean_squared_error: 1.2873 - val_loss: 0.8142 - val_root_mean_squared_error: 0.8191 - lr: 1.0000e-04\n",
      "Epoch 42/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.7963 - root_mean_squared_error: 1.2844\n",
      "Epoch 42: val_loss did not improve from 0.65053\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.7964 - root_mean_squared_error: 1.2844 - val_loss: 0.6966 - val_root_mean_squared_error: 0.7407 - lr: 1.0000e-04\n",
      "Epoch 43/500\n",
      "10809/10811 [============================>.] - ETA: 0s - loss: 1.7901 - root_mean_squared_error: 1.2816\n",
      "Epoch 43: val_loss did not improve from 0.65053\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.7903 - root_mean_squared_error: 1.2816 - val_loss: 0.7988 - val_root_mean_squared_error: 0.8042 - lr: 1.0000e-04\n",
      "Epoch 44/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.7804 - root_mean_squared_error: 1.2771\n",
      "Epoch 44: val_loss did not improve from 0.65053\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.7804 - root_mean_squared_error: 1.2771 - val_loss: 0.7647 - val_root_mean_squared_error: 0.7844 - lr: 1.0000e-04\n",
      "Epoch 45/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 1.7795 - root_mean_squared_error: 1.2760\n",
      "Epoch 45: val_loss did not improve from 0.65053\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.7795 - root_mean_squared_error: 1.2760 - val_loss: 0.7171 - val_root_mean_squared_error: 0.7538 - lr: 1.0000e-04\n",
      "Epoch 46/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.7711 - root_mean_squared_error: 1.2729\n",
      "Epoch 46: val_loss did not improve from 0.65053\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.7711 - root_mean_squared_error: 1.2730 - val_loss: 0.7206 - val_root_mean_squared_error: 0.7496 - lr: 1.0000e-04\n",
      "Epoch 47/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.7679 - root_mean_squared_error: 1.2714\n",
      "Epoch 47: val_loss did not improve from 0.65053\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.7680 - root_mean_squared_error: 1.2714 - val_loss: 0.7113 - val_root_mean_squared_error: 0.7480 - lr: 1.0000e-04\n",
      "Epoch 48/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 1.7637 - root_mean_squared_error: 1.2697\n",
      "Epoch 48: val_loss did not improve from 0.65053\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.7637 - root_mean_squared_error: 1.2697 - val_loss: 0.7567 - val_root_mean_squared_error: 0.7786 - lr: 1.0000e-04\n",
      "Epoch 49/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.7612 - root_mean_squared_error: 1.2688\n",
      "Epoch 49: val_loss did not improve from 0.65053\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.7612 - root_mean_squared_error: 1.2688 - val_loss: 0.7194 - val_root_mean_squared_error: 0.7549 - lr: 1.0000e-04\n",
      "Epoch 50/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.7607 - root_mean_squared_error: 1.2685\n",
      "Epoch 50: val_loss did not improve from 0.65053\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.7607 - root_mean_squared_error: 1.2685 - val_loss: 0.7267 - val_root_mean_squared_error: 0.7600 - lr: 1.0000e-04\n",
      "Epoch 51/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.7550 - root_mean_squared_error: 1.2661\n",
      "Epoch 51: val_loss did not improve from 0.65053\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.7550 - root_mean_squared_error: 1.2661 - val_loss: 0.6599 - val_root_mean_squared_error: 0.7130 - lr: 1.0000e-04\n",
      "Epoch 52/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.7479 - root_mean_squared_error: 1.2633\n",
      "Epoch 52: val_loss did not improve from 0.65053\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.7478 - root_mean_squared_error: 1.2633 - val_loss: 0.6632 - val_root_mean_squared_error: 0.7161 - lr: 1.0000e-04\n",
      "Epoch 53/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.7514 - root_mean_squared_error: 1.2644\n",
      "Epoch 53: val_loss did not improve from 0.65053\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.7514 - root_mean_squared_error: 1.2643 - val_loss: 0.7487 - val_root_mean_squared_error: 0.7700 - lr: 1.0000e-04\n",
      "Epoch 54/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 1.5610 - root_mean_squared_error: 1.2095\n",
      "Epoch 54: val_loss improved from 0.65053 to 0.64186, saving model to .mdl_wts/\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.5610 - root_mean_squared_error: 1.2095 - val_loss: 0.6419 - val_root_mean_squared_error: 0.7479 - lr: 1.0000e-05\n",
      "Epoch 55/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.4915 - root_mean_squared_error: 1.1884\n",
      "Epoch 55: val_loss did not improve from 0.64186\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.4913 - root_mean_squared_error: 1.1883 - val_loss: 0.6420 - val_root_mean_squared_error: 0.7520 - lr: 1.0000e-05\n",
      "Epoch 56/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.4681 - root_mean_squared_error: 1.1805\n",
      "Epoch 56: val_loss did not improve from 0.64186\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.4681 - root_mean_squared_error: 1.1805 - val_loss: 0.6462 - val_root_mean_squared_error: 0.7573 - lr: 1.0000e-05\n",
      "Epoch 57/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.4552 - root_mean_squared_error: 1.1764\n",
      "Epoch 57: val_loss improved from 0.64186 to 0.63771, saving model to .mdl_wts/\n",
      "10811/10811 [==============================] - 86s 8ms/step - loss: 1.4554 - root_mean_squared_error: 1.1764 - val_loss: 0.6377 - val_root_mean_squared_error: 0.7536 - lr: 1.0000e-05\n",
      "Epoch 58/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.4521 - root_mean_squared_error: 1.1761\n",
      "Epoch 58: val_loss improved from 0.63771 to 0.62915, saving model to .mdl_wts/\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.4521 - root_mean_squared_error: 1.1761 - val_loss: 0.6292 - val_root_mean_squared_error: 0.7493 - lr: 1.0000e-05\n",
      "Epoch 59/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.4409 - root_mean_squared_error: 1.1723\n",
      "Epoch 59: val_loss did not improve from 0.62915\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.4409 - root_mean_squared_error: 1.1722 - val_loss: 0.6334 - val_root_mean_squared_error: 0.7536 - lr: 1.0000e-05\n",
      "Epoch 60/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.4420 - root_mean_squared_error: 1.1735\n",
      "Epoch 60: val_loss did not improve from 0.62915\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.4420 - root_mean_squared_error: 1.1735 - val_loss: 0.6406 - val_root_mean_squared_error: 0.7594 - lr: 1.0000e-05\n",
      "Epoch 61/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.4402 - root_mean_squared_error: 1.1734\n",
      "Epoch 61: val_loss did not improve from 0.62915\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.4401 - root_mean_squared_error: 1.1734 - val_loss: 0.6391 - val_root_mean_squared_error: 0.7594 - lr: 1.0000e-05\n",
      "Epoch 62/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.4369 - root_mean_squared_error: 1.1726\n",
      "Epoch 62: val_loss did not improve from 0.62915\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.4369 - root_mean_squared_error: 1.1726 - val_loss: 0.6444 - val_root_mean_squared_error: 0.7638 - lr: 1.0000e-05\n",
      "Epoch 63/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.4353 - root_mean_squared_error: 1.1725\n",
      "Epoch 63: val_loss did not improve from 0.62915\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.4353 - root_mean_squared_error: 1.1724 - val_loss: 0.6306 - val_root_mean_squared_error: 0.7555 - lr: 1.0000e-05\n",
      "Epoch 64/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.4315 - root_mean_squared_error: 1.1713\n",
      "Epoch 64: val_loss did not improve from 0.62915\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.4315 - root_mean_squared_error: 1.1713 - val_loss: 0.6296 - val_root_mean_squared_error: 0.7556 - lr: 1.0000e-05\n",
      "Epoch 65/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 1.4312 - root_mean_squared_error: 1.1716\n",
      "Epoch 65: val_loss did not improve from 0.62915\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.4312 - root_mean_squared_error: 1.1716 - val_loss: 0.6313 - val_root_mean_squared_error: 0.7573 - lr: 1.0000e-05\n",
      "Epoch 66/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 1.4317 - root_mean_squared_error: 1.1722\n",
      "Epoch 66: val_loss improved from 0.62915 to 0.62607, saving model to .mdl_wts/\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.4317 - root_mean_squared_error: 1.1722 - val_loss: 0.6261 - val_root_mean_squared_error: 0.7544 - lr: 1.0000e-05\n",
      "Epoch 67/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.4297 - root_mean_squared_error: 1.1717\n",
      "Epoch 67: val_loss did not improve from 0.62607\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.4298 - root_mean_squared_error: 1.1717 - val_loss: 0.6300 - val_root_mean_squared_error: 0.7576 - lr: 1.0000e-05\n",
      "Epoch 68/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.4323 - root_mean_squared_error: 1.1731\n",
      "Epoch 68: val_loss did not improve from 0.62607\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.4322 - root_mean_squared_error: 1.1731 - val_loss: 0.6339 - val_root_mean_squared_error: 0.7605 - lr: 1.0000e-05\n",
      "Epoch 69/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.4244 - root_mean_squared_error: 1.1700\n",
      "Epoch 69: val_loss improved from 0.62607 to 0.62557, saving model to .mdl_wts/\n",
      "10811/10811 [==============================] - 86s 8ms/step - loss: 1.4244 - root_mean_squared_error: 1.1700 - val_loss: 0.6256 - val_root_mean_squared_error: 0.7555 - lr: 1.0000e-05\n",
      "Epoch 70/500\n",
      "10809/10811 [============================>.] - ETA: 0s - loss: 1.4270 - root_mean_squared_error: 1.1715\n",
      "Epoch 70: val_loss did not improve from 0.62557\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.4270 - root_mean_squared_error: 1.1714 - val_loss: 0.6334 - val_root_mean_squared_error: 0.7611 - lr: 1.0000e-05\n",
      "Epoch 71/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.4244 - root_mean_squared_error: 1.1706\n",
      "Epoch 71: val_loss improved from 0.62557 to 0.62226, saving model to .mdl_wts/\n",
      "10811/10811 [==============================] - 86s 8ms/step - loss: 1.4244 - root_mean_squared_error: 1.1706 - val_loss: 0.6223 - val_root_mean_squared_error: 0.7541 - lr: 1.0000e-05\n",
      "Epoch 72/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.4220 - root_mean_squared_error: 1.1698\n",
      "Epoch 72: val_loss did not improve from 0.62226\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.4220 - root_mean_squared_error: 1.1698 - val_loss: 0.6259 - val_root_mean_squared_error: 0.7570 - lr: 1.0000e-05\n",
      "Epoch 73/500\n",
      "10809/10811 [============================>.] - ETA: 0s - loss: 1.4218 - root_mean_squared_error: 1.1699\n",
      "Epoch 73: val_loss improved from 0.62226 to 0.61975, saving model to .mdl_wts/\n",
      "10811/10811 [==============================] - 86s 8ms/step - loss: 1.4218 - root_mean_squared_error: 1.1700 - val_loss: 0.6197 - val_root_mean_squared_error: 0.7531 - lr: 1.0000e-05\n",
      "Epoch 74/500\n",
      "10809/10811 [============================>.] - ETA: 0s - loss: 1.4230 - root_mean_squared_error: 1.1707\n",
      "Epoch 74: val_loss did not improve from 0.61975\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.4230 - root_mean_squared_error: 1.1707 - val_loss: 0.6229 - val_root_mean_squared_error: 0.7556 - lr: 1.0000e-05\n",
      "Epoch 75/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 1.4198 - root_mean_squared_error: 1.1695\n",
      "Epoch 75: val_loss did not improve from 0.61975\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.4198 - root_mean_squared_error: 1.1695 - val_loss: 0.6241 - val_root_mean_squared_error: 0.7566 - lr: 1.0000e-05\n",
      "Epoch 76/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.4206 - root_mean_squared_error: 1.1700\n",
      "Epoch 76: val_loss improved from 0.61975 to 0.61489, saving model to .mdl_wts/\n",
      "10811/10811 [==============================] - 86s 8ms/step - loss: 1.4206 - root_mean_squared_error: 1.1700 - val_loss: 0.6149 - val_root_mean_squared_error: 0.7507 - lr: 1.0000e-05\n",
      "Epoch 77/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.4198 - root_mean_squared_error: 1.1698\n",
      "Epoch 77: val_loss did not improve from 0.61489\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.4198 - root_mean_squared_error: 1.1698 - val_loss: 0.6259 - val_root_mean_squared_error: 0.7581 - lr: 1.0000e-05\n",
      "Epoch 78/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.4167 - root_mean_squared_error: 1.1686\n",
      "Epoch 78: val_loss did not improve from 0.61489\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.4166 - root_mean_squared_error: 1.1686 - val_loss: 0.6174 - val_root_mean_squared_error: 0.7530 - lr: 1.0000e-05\n",
      "Epoch 79/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.4194 - root_mean_squared_error: 1.1700\n",
      "Epoch 79: val_loss improved from 0.61489 to 0.61353, saving model to .mdl_wts/\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.4195 - root_mean_squared_error: 1.1700 - val_loss: 0.6135 - val_root_mean_squared_error: 0.7506 - lr: 1.0000e-05\n",
      "Epoch 80/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 1.4146 - root_mean_squared_error: 1.1680\n",
      "Epoch 80: val_loss did not improve from 0.61353\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.4146 - root_mean_squared_error: 1.1680 - val_loss: 0.6198 - val_root_mean_squared_error: 0.7550 - lr: 1.0000e-05\n",
      "Epoch 81/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.4148 - root_mean_squared_error: 1.1683\n",
      "Epoch 81: val_loss did not improve from 0.61353\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.4147 - root_mean_squared_error: 1.1682 - val_loss: 0.6245 - val_root_mean_squared_error: 0.7582 - lr: 1.0000e-05\n",
      "Epoch 82/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.4118 - root_mean_squared_error: 1.1671\n",
      "Epoch 82: val_loss did not improve from 0.61353\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.4118 - root_mean_squared_error: 1.1671 - val_loss: 0.6247 - val_root_mean_squared_error: 0.7585 - lr: 1.0000e-05\n",
      "Epoch 83/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.4130 - root_mean_squared_error: 1.1677\n",
      "Epoch 83: val_loss did not improve from 0.61353\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.4129 - root_mean_squared_error: 1.1677 - val_loss: 0.6262 - val_root_mean_squared_error: 0.7597 - lr: 1.0000e-05\n",
      "Epoch 84/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.4110 - root_mean_squared_error: 1.1670\n",
      "Epoch 84: val_loss did not improve from 0.61353\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.4112 - root_mean_squared_error: 1.1671 - val_loss: 0.6182 - val_root_mean_squared_error: 0.7546 - lr: 1.0000e-05\n",
      "Epoch 85/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.4105 - root_mean_squared_error: 1.1669\n",
      "Epoch 85: val_loss did not improve from 0.61353\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.4110 - root_mean_squared_error: 1.1671 - val_loss: 0.6251 - val_root_mean_squared_error: 0.7593 - lr: 1.0000e-05\n",
      "Epoch 86/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.4076 - root_mean_squared_error: 1.1657\n",
      "Epoch 86: val_loss did not improve from 0.61353\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.4075 - root_mean_squared_error: 1.1657 - val_loss: 0.6266 - val_root_mean_squared_error: 0.7604 - lr: 1.0000e-05\n",
      "Epoch 87/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.4096 - root_mean_squared_error: 1.1666\n",
      "Epoch 87: val_loss improved from 0.61353 to 0.60412, saving model to .mdl_wts/\n",
      "10811/10811 [==============================] - 86s 8ms/step - loss: 1.4095 - root_mean_squared_error: 1.1666 - val_loss: 0.6041 - val_root_mean_squared_error: 0.7455 - lr: 1.0000e-05\n",
      "Epoch 88/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.4096 - root_mean_squared_error: 1.1667\n",
      "Epoch 88: val_loss did not improve from 0.60412\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.4100 - root_mean_squared_error: 1.1669 - val_loss: 0.6153 - val_root_mean_squared_error: 0.7533 - lr: 1.0000e-05\n",
      "Epoch 89/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.4088 - root_mean_squared_error: 1.1664\n",
      "Epoch 89: val_loss did not improve from 0.60412\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.4087 - root_mean_squared_error: 1.1664 - val_loss: 0.6276 - val_root_mean_squared_error: 0.7615 - lr: 1.0000e-05\n",
      "Epoch 90/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.4063 - root_mean_squared_error: 1.1655\n",
      "Epoch 90: val_loss did not improve from 0.60412\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.4063 - root_mean_squared_error: 1.1655 - val_loss: 0.6187 - val_root_mean_squared_error: 0.7557 - lr: 1.0000e-05\n",
      "Epoch 91/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 1.4052 - root_mean_squared_error: 1.1651\n",
      "Epoch 91: val_loss did not improve from 0.60412\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.4052 - root_mean_squared_error: 1.1651 - val_loss: 0.6140 - val_root_mean_squared_error: 0.7528 - lr: 1.0000e-05\n",
      "Epoch 92/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.4043 - root_mean_squared_error: 1.1648\n",
      "Epoch 92: val_loss did not improve from 0.60412\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.4042 - root_mean_squared_error: 1.1648 - val_loss: 0.6252 - val_root_mean_squared_error: 0.7602 - lr: 1.0000e-05\n",
      "Epoch 93/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.4051 - root_mean_squared_error: 1.1653\n",
      "Epoch 93: val_loss improved from 0.60412 to 0.60219, saving model to .mdl_wts/\n",
      "10811/10811 [==============================] - 86s 8ms/step - loss: 1.4052 - root_mean_squared_error: 1.1653 - val_loss: 0.6022 - val_root_mean_squared_error: 0.7450 - lr: 1.0000e-05\n",
      "Epoch 94/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 1.4022 - root_mean_squared_error: 1.1640\n",
      "Epoch 94: val_loss did not improve from 0.60219\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.4022 - root_mean_squared_error: 1.1640 - val_loss: 0.6181 - val_root_mean_squared_error: 0.7558 - lr: 1.0000e-05\n",
      "Epoch 95/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.4008 - root_mean_squared_error: 1.1635\n",
      "Epoch 95: val_loss did not improve from 0.60219\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.4006 - root_mean_squared_error: 1.1634 - val_loss: 0.6065 - val_root_mean_squared_error: 0.7480 - lr: 1.0000e-05\n",
      "Epoch 96/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.4010 - root_mean_squared_error: 1.1636\n",
      "Epoch 96: val_loss did not improve from 0.60219\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.4008 - root_mean_squared_error: 1.1635 - val_loss: 0.6146 - val_root_mean_squared_error: 0.7536 - lr: 1.0000e-05\n",
      "Epoch 97/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 1.3992 - root_mean_squared_error: 1.1630\n",
      "Epoch 97: val_loss did not improve from 0.60219\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3992 - root_mean_squared_error: 1.1630 - val_loss: 0.6133 - val_root_mean_squared_error: 0.7529 - lr: 1.0000e-05\n",
      "Epoch 98/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 1.3994 - root_mean_squared_error: 1.1631\n",
      "Epoch 98: val_loss improved from 0.60219 to 0.59954, saving model to .mdl_wts/\n",
      "10811/10811 [==============================] - 86s 8ms/step - loss: 1.3994 - root_mean_squared_error: 1.1631 - val_loss: 0.5995 - val_root_mean_squared_error: 0.7437 - lr: 1.0000e-05\n",
      "Epoch 99/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.4011 - root_mean_squared_error: 1.1638\n",
      "Epoch 99: val_loss did not improve from 0.59954\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.4013 - root_mean_squared_error: 1.1639 - val_loss: 0.6203 - val_root_mean_squared_error: 0.7576 - lr: 1.0000e-05\n",
      "Epoch 100/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.3994 - root_mean_squared_error: 1.1631\n",
      "Epoch 100: val_loss did not improve from 0.59954\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3994 - root_mean_squared_error: 1.1631 - val_loss: 0.6127 - val_root_mean_squared_error: 0.7527 - lr: 1.0000e-05\n",
      "Epoch 101/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.3954 - root_mean_squared_error: 1.1614\n",
      "Epoch 101: val_loss did not improve from 0.59954\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3954 - root_mean_squared_error: 1.1614 - val_loss: 0.6009 - val_root_mean_squared_error: 0.7448 - lr: 1.0000e-05\n",
      "Epoch 102/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.3937 - root_mean_squared_error: 1.1608\n",
      "Epoch 102: val_loss did not improve from 0.59954\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3937 - root_mean_squared_error: 1.1608 - val_loss: 0.6192 - val_root_mean_squared_error: 0.7571 - lr: 1.0000e-05\n",
      "Epoch 103/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.3905 - root_mean_squared_error: 1.1595\n",
      "Epoch 103: val_loss did not improve from 0.59954\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3904 - root_mean_squared_error: 1.1594 - val_loss: 0.6085 - val_root_mean_squared_error: 0.7501 - lr: 1.0000e-05\n",
      "Epoch 104/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.3922 - root_mean_squared_error: 1.1602\n",
      "Epoch 104: val_loss did not improve from 0.59954\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3921 - root_mean_squared_error: 1.1602 - val_loss: 0.6038 - val_root_mean_squared_error: 0.7471 - lr: 1.0000e-05\n",
      "Epoch 105/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 1.3918 - root_mean_squared_error: 1.1601\n",
      "Epoch 105: val_loss did not improve from 0.59954\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3918 - root_mean_squared_error: 1.1601 - val_loss: 0.6110 - val_root_mean_squared_error: 0.7517 - lr: 1.0000e-05\n",
      "Epoch 106/500\n",
      "10809/10811 [============================>.] - ETA: 0s - loss: 1.3899 - root_mean_squared_error: 1.1593\n",
      "Epoch 106: val_loss did not improve from 0.59954\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3899 - root_mean_squared_error: 1.1593 - val_loss: 0.5999 - val_root_mean_squared_error: 0.7445 - lr: 1.0000e-05\n",
      "Epoch 107/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.3885 - root_mean_squared_error: 1.1588\n",
      "Epoch 107: val_loss did not improve from 0.59954\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3886 - root_mean_squared_error: 1.1588 - val_loss: 0.6162 - val_root_mean_squared_error: 0.7554 - lr: 1.0000e-05\n",
      "Epoch 108/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 1.3847 - root_mean_squared_error: 1.1572\n",
      "Epoch 108: val_loss did not improve from 0.59954\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3847 - root_mean_squared_error: 1.1572 - val_loss: 0.6112 - val_root_mean_squared_error: 0.7523 - lr: 1.0000e-05\n",
      "Epoch 109/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.3880 - root_mean_squared_error: 1.1587\n",
      "Epoch 109: val_loss did not improve from 0.59954\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3880 - root_mean_squared_error: 1.1587 - val_loss: 0.6068 - val_root_mean_squared_error: 0.7492 - lr: 1.0000e-05\n",
      "Epoch 110/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.3855 - root_mean_squared_error: 1.1575\n",
      "Epoch 110: val_loss did not improve from 0.59954\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3855 - root_mean_squared_error: 1.1575 - val_loss: 0.6041 - val_root_mean_squared_error: 0.7476 - lr: 1.0000e-05\n",
      "Epoch 111/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.3867 - root_mean_squared_error: 1.1581\n",
      "Epoch 111: val_loss did not improve from 0.59954\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3867 - root_mean_squared_error: 1.1581 - val_loss: 0.6002 - val_root_mean_squared_error: 0.7450 - lr: 1.0000e-05\n",
      "Epoch 112/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.3860 - root_mean_squared_error: 1.1578\n",
      "Epoch 112: val_loss did not improve from 0.59954\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3859 - root_mean_squared_error: 1.1578 - val_loss: 0.6069 - val_root_mean_squared_error: 0.7494 - lr: 1.0000e-05\n",
      "Epoch 113/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.3823 - root_mean_squared_error: 1.1562\n",
      "Epoch 113: val_loss did not improve from 0.59954\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3822 - root_mean_squared_error: 1.1562 - val_loss: 0.6073 - val_root_mean_squared_error: 0.7498 - lr: 1.0000e-05\n",
      "Epoch 114/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.3833 - root_mean_squared_error: 1.1567\n",
      "Epoch 114: val_loss did not improve from 0.59954\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3831 - root_mean_squared_error: 1.1566 - val_loss: 0.6143 - val_root_mean_squared_error: 0.7545 - lr: 1.0000e-05\n",
      "Epoch 115/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.3821 - root_mean_squared_error: 1.1562\n",
      "Epoch 115: val_loss did not improve from 0.59954\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3820 - root_mean_squared_error: 1.1562 - val_loss: 0.6136 - val_root_mean_squared_error: 0.7539 - lr: 1.0000e-05\n",
      "Epoch 116/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.3846 - root_mean_squared_error: 1.1573\n",
      "Epoch 116: val_loss did not improve from 0.59954\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3845 - root_mean_squared_error: 1.1572 - val_loss: 0.6111 - val_root_mean_squared_error: 0.7522 - lr: 1.0000e-05\n",
      "Epoch 117/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.3821 - root_mean_squared_error: 1.1562\n",
      "Epoch 117: val_loss did not improve from 0.59954\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3821 - root_mean_squared_error: 1.1562 - val_loss: 0.6038 - val_root_mean_squared_error: 0.7476 - lr: 1.0000e-05\n",
      "Epoch 118/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.3802 - root_mean_squared_error: 1.1554\n",
      "Epoch 118: val_loss did not improve from 0.59954\n",
      "\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3805 - root_mean_squared_error: 1.1555 - val_loss: 0.6135 - val_root_mean_squared_error: 0.7540 - lr: 1.0000e-05\n",
      "Epoch 119/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.3304 - root_mean_squared_error: 1.1347\n",
      "Epoch 119: val_loss did not improve from 0.59954\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3304 - root_mean_squared_error: 1.1347 - val_loss: 0.6084 - val_root_mean_squared_error: 0.7532 - lr: 1.0000e-06\n",
      "Epoch 120/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 1.3221 - root_mean_squared_error: 1.1324\n",
      "Epoch 120: val_loss did not improve from 0.59954\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3221 - root_mean_squared_error: 1.1324 - val_loss: 0.6024 - val_root_mean_squared_error: 0.7506 - lr: 1.0000e-06\n",
      "Epoch 121/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.3202 - root_mean_squared_error: 1.1322\n",
      "Epoch 121: val_loss did not improve from 0.59954\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3202 - root_mean_squared_error: 1.1321 - val_loss: 0.6044 - val_root_mean_squared_error: 0.7526 - lr: 1.0000e-06\n",
      "Epoch 122/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.3156 - root_mean_squared_error: 1.1305\n",
      "Epoch 122: val_loss did not improve from 0.59954\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3156 - root_mean_squared_error: 1.1305 - val_loss: 0.6056 - val_root_mean_squared_error: 0.7538 - lr: 1.0000e-06\n",
      "Epoch 123/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.3125 - root_mean_squared_error: 1.1293\n",
      "Epoch 123: val_loss improved from 0.59954 to 0.59743, saving model to .mdl_wts/\n",
      "10811/10811 [==============================] - 86s 8ms/step - loss: 1.3125 - root_mean_squared_error: 1.1293 - val_loss: 0.5974 - val_root_mean_squared_error: 0.7487 - lr: 1.0000e-06\n",
      "Epoch 124/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.3143 - root_mean_squared_error: 1.1303\n",
      "Epoch 124: val_loss did not improve from 0.59743\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3143 - root_mean_squared_error: 1.1303 - val_loss: 0.5982 - val_root_mean_squared_error: 0.7494 - lr: 1.0000e-06\n",
      "Epoch 125/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.3116 - root_mean_squared_error: 1.1292\n",
      "Epoch 125: val_loss did not improve from 0.59743\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3114 - root_mean_squared_error: 1.1291 - val_loss: 0.6013 - val_root_mean_squared_error: 0.7516 - lr: 1.0000e-06\n",
      "Epoch 126/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.3092 - root_mean_squared_error: 1.1283\n",
      "Epoch 126: val_loss improved from 0.59743 to 0.59653, saving model to .mdl_wts/\n",
      "10811/10811 [==============================] - 86s 8ms/step - loss: 1.3092 - root_mean_squared_error: 1.1282 - val_loss: 0.5965 - val_root_mean_squared_error: 0.7486 - lr: 1.0000e-06\n",
      "Epoch 127/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.3074 - root_mean_squared_error: 1.1275\n",
      "Epoch 127: val_loss did not improve from 0.59653\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3075 - root_mean_squared_error: 1.1276 - val_loss: 0.6020 - val_root_mean_squared_error: 0.7523 - lr: 1.0000e-06\n",
      "Epoch 128/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.3097 - root_mean_squared_error: 1.1286\n",
      "Epoch 128: val_loss did not improve from 0.59653\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3099 - root_mean_squared_error: 1.1287 - val_loss: 0.6010 - val_root_mean_squared_error: 0.7518 - lr: 1.0000e-06\n",
      "Epoch 129/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.3103 - root_mean_squared_error: 1.1289\n",
      "Epoch 129: val_loss did not improve from 0.59653\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3103 - root_mean_squared_error: 1.1289 - val_loss: 0.5975 - val_root_mean_squared_error: 0.7495 - lr: 1.0000e-06\n",
      "Epoch 130/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.3096 - root_mean_squared_error: 1.1287\n",
      "Epoch 130: val_loss did not improve from 0.59653\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3095 - root_mean_squared_error: 1.1286 - val_loss: 0.6019 - val_root_mean_squared_error: 0.7525 - lr: 1.0000e-06\n",
      "Epoch 131/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.3079 - root_mean_squared_error: 1.1279\n",
      "Epoch 131: val_loss did not improve from 0.59653\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3077 - root_mean_squared_error: 1.1279 - val_loss: 0.5992 - val_root_mean_squared_error: 0.7508 - lr: 1.0000e-06\n",
      "Epoch 132/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.3098 - root_mean_squared_error: 1.1288\n",
      "Epoch 132: val_loss did not improve from 0.59653\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3098 - root_mean_squared_error: 1.1288 - val_loss: 0.6011 - val_root_mean_squared_error: 0.7521 - lr: 1.0000e-06\n",
      "Epoch 133/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.3057 - root_mean_squared_error: 1.1271\n",
      "Epoch 133: val_loss did not improve from 0.59653\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3056 - root_mean_squared_error: 1.1270 - val_loss: 0.5997 - val_root_mean_squared_error: 0.7512 - lr: 1.0000e-06\n",
      "Epoch 134/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.3042 - root_mean_squared_error: 1.1264\n",
      "Epoch 134: val_loss did not improve from 0.59653\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3042 - root_mean_squared_error: 1.1264 - val_loss: 0.5998 - val_root_mean_squared_error: 0.7514 - lr: 1.0000e-06\n",
      "Epoch 135/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.3085 - root_mean_squared_error: 1.1284\n",
      "Epoch 135: val_loss did not improve from 0.59653\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3085 - root_mean_squared_error: 1.1284 - val_loss: 0.5994 - val_root_mean_squared_error: 0.7511 - lr: 1.0000e-06\n",
      "Epoch 136/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.3081 - root_mean_squared_error: 1.1282\n",
      "Epoch 136: val_loss did not improve from 0.59653\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3081 - root_mean_squared_error: 1.1282 - val_loss: 0.5991 - val_root_mean_squared_error: 0.7509 - lr: 1.0000e-06\n",
      "Epoch 137/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.3056 - root_mean_squared_error: 1.1271\n",
      "Epoch 137: val_loss did not improve from 0.59653\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3055 - root_mean_squared_error: 1.1271 - val_loss: 0.6021 - val_root_mean_squared_error: 0.7530 - lr: 1.0000e-06\n",
      "Epoch 138/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.3026 - root_mean_squared_error: 1.1258\n",
      "Epoch 138: val_loss did not improve from 0.59653\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3026 - root_mean_squared_error: 1.1258 - val_loss: 0.6036 - val_root_mean_squared_error: 0.7540 - lr: 1.0000e-06\n",
      "Epoch 139/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 1.3070 - root_mean_squared_error: 1.1278\n",
      "Epoch 139: val_loss did not improve from 0.59653\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3070 - root_mean_squared_error: 1.1278 - val_loss: 0.5994 - val_root_mean_squared_error: 0.7513 - lr: 1.0000e-06\n",
      "Epoch 140/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.3038 - root_mean_squared_error: 1.1265\n",
      "Epoch 140: val_loss did not improve from 0.59653\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3038 - root_mean_squared_error: 1.1265 - val_loss: 0.6034 - val_root_mean_squared_error: 0.7540 - lr: 1.0000e-06\n",
      "Epoch 141/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.3048 - root_mean_squared_error: 1.1269\n",
      "Epoch 141: val_loss improved from 0.59653 to 0.59650, saving model to .mdl_wts/\n",
      "10811/10811 [==============================] - 86s 8ms/step - loss: 1.3049 - root_mean_squared_error: 1.1269 - val_loss: 0.5965 - val_root_mean_squared_error: 0.7494 - lr: 1.0000e-06\n",
      "Epoch 142/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.3044 - root_mean_squared_error: 1.1268\n",
      "Epoch 142: val_loss did not improve from 0.59650\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3044 - root_mean_squared_error: 1.1267 - val_loss: 0.5967 - val_root_mean_squared_error: 0.7496 - lr: 1.0000e-06\n",
      "Epoch 143/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.3059 - root_mean_squared_error: 1.1274\n",
      "Epoch 143: val_loss did not improve from 0.59650\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3058 - root_mean_squared_error: 1.1274 - val_loss: 0.5999 - val_root_mean_squared_error: 0.7517 - lr: 1.0000e-06\n",
      "Epoch 144/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.3052 - root_mean_squared_error: 1.1271\n",
      "Epoch 144: val_loss did not improve from 0.59650\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3050 - root_mean_squared_error: 1.1271 - val_loss: 0.5966 - val_root_mean_squared_error: 0.7496 - lr: 1.0000e-06\n",
      "Epoch 145/500\n",
      "10809/10811 [============================>.] - ETA: 0s - loss: 1.3068 - root_mean_squared_error: 1.1279\n",
      "Epoch 145: val_loss improved from 0.59650 to 0.59594, saving model to .mdl_wts/\n",
      "10811/10811 [==============================] - 86s 8ms/step - loss: 1.3068 - root_mean_squared_error: 1.1279 - val_loss: 0.5959 - val_root_mean_squared_error: 0.7492 - lr: 1.0000e-06\n",
      "Epoch 146/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 1.3066 - root_mean_squared_error: 1.1278\n",
      "Epoch 146: val_loss did not improve from 0.59594\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3066 - root_mean_squared_error: 1.1278 - val_loss: 0.6010 - val_root_mean_squared_error: 0.7526 - lr: 1.0000e-06\n",
      "Epoch 147/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.3081 - root_mean_squared_error: 1.1285\n",
      "Epoch 147: val_loss did not improve from 0.59594\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3080 - root_mean_squared_error: 1.1284 - val_loss: 0.5974 - val_root_mean_squared_error: 0.7502 - lr: 1.0000e-06\n",
      "Epoch 148/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.3037 - root_mean_squared_error: 1.1266\n",
      "Epoch 148: val_loss did not improve from 0.59594\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3037 - root_mean_squared_error: 1.1265 - val_loss: 0.5981 - val_root_mean_squared_error: 0.7507 - lr: 1.0000e-06\n",
      "Epoch 149/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.3050 - root_mean_squared_error: 1.1272\n",
      "Epoch 149: val_loss did not improve from 0.59594\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3053 - root_mean_squared_error: 1.1273 - val_loss: 0.6003 - val_root_mean_squared_error: 0.7522 - lr: 1.0000e-06\n",
      "Epoch 150/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.3048 - root_mean_squared_error: 1.1271\n",
      "Epoch 150: val_loss did not improve from 0.59594\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3047 - root_mean_squared_error: 1.1271 - val_loss: 0.5979 - val_root_mean_squared_error: 0.7507 - lr: 1.0000e-06\n",
      "Epoch 151/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.3053 - root_mean_squared_error: 1.1273\n",
      "Epoch 151: val_loss did not improve from 0.59594\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3050 - root_mean_squared_error: 1.1272 - val_loss: 0.5977 - val_root_mean_squared_error: 0.7506 - lr: 1.0000e-06\n",
      "Epoch 152/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 1.3043 - root_mean_squared_error: 1.1269\n",
      "Epoch 152: val_loss did not improve from 0.59594\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3043 - root_mean_squared_error: 1.1269 - val_loss: 0.5968 - val_root_mean_squared_error: 0.7500 - lr: 1.0000e-06\n",
      "Epoch 153/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.3055 - root_mean_squared_error: 1.1274\n",
      "Epoch 153: val_loss did not improve from 0.59594\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3054 - root_mean_squared_error: 1.1274 - val_loss: 0.5999 - val_root_mean_squared_error: 0.7521 - lr: 1.0000e-06\n",
      "Epoch 154/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.3022 - root_mean_squared_error: 1.1260\n",
      "Epoch 154: val_loss did not improve from 0.59594\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3022 - root_mean_squared_error: 1.1260 - val_loss: 0.5998 - val_root_mean_squared_error: 0.7520 - lr: 1.0000e-06\n",
      "Epoch 155/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.3033 - root_mean_squared_error: 1.1265\n",
      "Epoch 155: val_loss did not improve from 0.59594\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3034 - root_mean_squared_error: 1.1265 - val_loss: 0.5980 - val_root_mean_squared_error: 0.7509 - lr: 1.0000e-06\n",
      "Epoch 156/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 1.3021 - root_mean_squared_error: 1.1260\n",
      "Epoch 156: val_loss did not improve from 0.59594\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3021 - root_mean_squared_error: 1.1260 - val_loss: 0.5959 - val_root_mean_squared_error: 0.7495 - lr: 1.0000e-06\n",
      "Epoch 157/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 1.3031 - root_mean_squared_error: 1.1264\n",
      "Epoch 157: val_loss did not improve from 0.59594\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3031 - root_mean_squared_error: 1.1264 - val_loss: 0.5987 - val_root_mean_squared_error: 0.7513 - lr: 1.0000e-06\n",
      "Epoch 158/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.2996 - root_mean_squared_error: 1.1249\n",
      "Epoch 158: val_loss did not improve from 0.59594\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2996 - root_mean_squared_error: 1.1249 - val_loss: 0.5992 - val_root_mean_squared_error: 0.7517 - lr: 1.0000e-06\n",
      "Epoch 159/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.3008 - root_mean_squared_error: 1.1255\n",
      "Epoch 159: val_loss did not improve from 0.59594\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3007 - root_mean_squared_error: 1.1254 - val_loss: 0.6006 - val_root_mean_squared_error: 0.7526 - lr: 1.0000e-06\n",
      "Epoch 160/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.3002 - root_mean_squared_error: 1.1252\n",
      "Epoch 160: val_loss did not improve from 0.59594\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3002 - root_mean_squared_error: 1.1252 - val_loss: 0.5993 - val_root_mean_squared_error: 0.7518 - lr: 1.0000e-06\n",
      "Epoch 161/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.3021 - root_mean_squared_error: 1.1261\n",
      "Epoch 161: val_loss did not improve from 0.59594\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3020 - root_mean_squared_error: 1.1261 - val_loss: 0.5974 - val_root_mean_squared_error: 0.7506 - lr: 1.0000e-06\n",
      "Epoch 162/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 1.3035 - root_mean_squared_error: 1.1267\n",
      "Epoch 162: val_loss did not improve from 0.59594\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3035 - root_mean_squared_error: 1.1267 - val_loss: 0.5972 - val_root_mean_squared_error: 0.7505 - lr: 1.0000e-06\n",
      "Epoch 163/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.3019 - root_mean_squared_error: 1.1260\n",
      "Epoch 163: val_loss did not improve from 0.59594\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3019 - root_mean_squared_error: 1.1260 - val_loss: 0.6002 - val_root_mean_squared_error: 0.7525 - lr: 1.0000e-06\n",
      "Epoch 164/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.2985 - root_mean_squared_error: 1.1245\n",
      "Epoch 164: val_loss did not improve from 0.59594\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2984 - root_mean_squared_error: 1.1245 - val_loss: 0.5990 - val_root_mean_squared_error: 0.7518 - lr: 1.0000e-06\n",
      "Epoch 165/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 1.3024 - root_mean_squared_error: 1.1263\n",
      "Epoch 165: val_loss did not improve from 0.59594\n",
      "\n",
      "Epoch 165: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.3024 - root_mean_squared_error: 1.1263 - val_loss: 0.5989 - val_root_mean_squared_error: 0.7517 - lr: 1.0000e-06\n",
      "Epoch 166/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.2944 - root_mean_squared_error: 1.1227\n",
      "Epoch 166: val_loss improved from 0.59594 to 0.59292, saving model to .mdl_wts/\n",
      "10811/10811 [==============================] - 86s 8ms/step - loss: 1.2944 - root_mean_squared_error: 1.1227 - val_loss: 0.5929 - val_root_mean_squared_error: 0.7477 - lr: 1.0000e-07\n",
      "Epoch 167/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.2922 - root_mean_squared_error: 1.1218\n",
      "Epoch 167: val_loss did not improve from 0.59292\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2922 - root_mean_squared_error: 1.1218 - val_loss: 0.5955 - val_root_mean_squared_error: 0.7495 - lr: 1.0000e-07\n",
      "Epoch 168/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.2910 - root_mean_squared_error: 1.1213\n",
      "Epoch 168: val_loss did not improve from 0.59292\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2912 - root_mean_squared_error: 1.1214 - val_loss: 0.5939 - val_root_mean_squared_error: 0.7485 - lr: 1.0000e-07\n",
      "Epoch 169/500\n",
      "10809/10811 [============================>.] - ETA: 0s - loss: 1.2966 - root_mean_squared_error: 1.1238\n",
      "Epoch 169: val_loss did not improve from 0.59292\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2966 - root_mean_squared_error: 1.1238 - val_loss: 0.5982 - val_root_mean_squared_error: 0.7514 - lr: 1.0000e-07\n",
      "Epoch 170/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.2952 - root_mean_squared_error: 1.1232\n",
      "Epoch 170: val_loss did not improve from 0.59292\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2951 - root_mean_squared_error: 1.1231 - val_loss: 0.5980 - val_root_mean_squared_error: 0.7512 - lr: 1.0000e-07\n",
      "Epoch 171/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.2937 - root_mean_squared_error: 1.1225\n",
      "Epoch 171: val_loss did not improve from 0.59292\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2938 - root_mean_squared_error: 1.1226 - val_loss: 0.5980 - val_root_mean_squared_error: 0.7513 - lr: 1.0000e-07\n",
      "Epoch 172/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.2877 - root_mean_squared_error: 1.1199\n",
      "Epoch 172: val_loss did not improve from 0.59292\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2878 - root_mean_squared_error: 1.1199 - val_loss: 0.5970 - val_root_mean_squared_error: 0.7506 - lr: 1.0000e-07\n",
      "Epoch 173/500\n",
      "10809/10811 [============================>.] - ETA: 0s - loss: 1.2956 - root_mean_squared_error: 1.1234\n",
      "Epoch 173: val_loss did not improve from 0.59292\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2957 - root_mean_squared_error: 1.1234 - val_loss: 0.5941 - val_root_mean_squared_error: 0.7487 - lr: 1.0000e-07\n",
      "Epoch 174/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 1.2920 - root_mean_squared_error: 1.1218\n",
      "Epoch 174: val_loss did not improve from 0.59292\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2920 - root_mean_squared_error: 1.1218 - val_loss: 0.5949 - val_root_mean_squared_error: 0.7492 - lr: 1.0000e-07\n",
      "Epoch 175/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 1.2927 - root_mean_squared_error: 1.1221\n",
      "Epoch 175: val_loss did not improve from 0.59292\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2927 - root_mean_squared_error: 1.1221 - val_loss: 0.5960 - val_root_mean_squared_error: 0.7500 - lr: 1.0000e-07\n",
      "Epoch 176/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.2923 - root_mean_squared_error: 1.1220\n",
      "Epoch 176: val_loss did not improve from 0.59292\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2921 - root_mean_squared_error: 1.1219 - val_loss: 0.5983 - val_root_mean_squared_error: 0.7515 - lr: 1.0000e-07\n",
      "Epoch 177/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.2947 - root_mean_squared_error: 1.1230\n",
      "Epoch 177: val_loss did not improve from 0.59292\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2947 - root_mean_squared_error: 1.1230 - val_loss: 0.5980 - val_root_mean_squared_error: 0.7514 - lr: 1.0000e-07\n",
      "Epoch 178/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 1.2922 - root_mean_squared_error: 1.1219\n",
      "Epoch 178: val_loss did not improve from 0.59292\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2922 - root_mean_squared_error: 1.1219 - val_loss: 0.5997 - val_root_mean_squared_error: 0.7525 - lr: 1.0000e-07\n",
      "Epoch 179/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.2916 - root_mean_squared_error: 1.1217\n",
      "Epoch 179: val_loss did not improve from 0.59292\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2915 - root_mean_squared_error: 1.1216 - val_loss: 0.6012 - val_root_mean_squared_error: 0.7535 - lr: 1.0000e-07\n",
      "Epoch 180/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.2902 - root_mean_squared_error: 1.1211\n",
      "Epoch 180: val_loss did not improve from 0.59292\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2902 - root_mean_squared_error: 1.1210 - val_loss: 0.5955 - val_root_mean_squared_error: 0.7497 - lr: 1.0000e-07\n",
      "Epoch 181/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.2941 - root_mean_squared_error: 1.1228\n",
      "Epoch 181: val_loss did not improve from 0.59292\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2940 - root_mean_squared_error: 1.1227 - val_loss: 0.5960 - val_root_mean_squared_error: 0.7500 - lr: 1.0000e-07\n",
      "Epoch 182/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.2910 - root_mean_squared_error: 1.1214\n",
      "Epoch 182: val_loss did not improve from 0.59292\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2909 - root_mean_squared_error: 1.1214 - val_loss: 0.5994 - val_root_mean_squared_error: 0.7523 - lr: 1.0000e-07\n",
      "Epoch 183/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.2938 - root_mean_squared_error: 1.1227\n",
      "Epoch 183: val_loss did not improve from 0.59292\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2938 - root_mean_squared_error: 1.1227 - val_loss: 0.5955 - val_root_mean_squared_error: 0.7498 - lr: 1.0000e-07\n",
      "Epoch 184/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.2939 - root_mean_squared_error: 1.1227\n",
      "Epoch 184: val_loss did not improve from 0.59292\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2943 - root_mean_squared_error: 1.1229 - val_loss: 0.5963 - val_root_mean_squared_error: 0.7503 - lr: 1.0000e-07\n",
      "Epoch 185/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.2906 - root_mean_squared_error: 1.1213\n",
      "Epoch 185: val_loss did not improve from 0.59292\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2905 - root_mean_squared_error: 1.1212 - val_loss: 0.5971 - val_root_mean_squared_error: 0.7509 - lr: 1.0000e-07\n",
      "Epoch 186/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.2904 - root_mean_squared_error: 1.1212\n",
      "Epoch 186: val_loss did not improve from 0.59292\n",
      "\n",
      "Epoch 186: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2907 - root_mean_squared_error: 1.1213 - val_loss: 0.5968 - val_root_mean_squared_error: 0.7506 - lr: 1.0000e-07\n",
      "Epoch 187/500\n",
      "10809/10811 [============================>.] - ETA: 0s - loss: 1.2915 - root_mean_squared_error: 1.1217\n",
      "Epoch 187: val_loss did not improve from 0.59292\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2915 - root_mean_squared_error: 1.1217 - val_loss: 0.6003 - val_root_mean_squared_error: 0.7530 - lr: 1.0000e-08\n",
      "Epoch 188/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.2883 - root_mean_squared_error: 1.1203\n",
      "Epoch 188: val_loss did not improve from 0.59292\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2882 - root_mean_squared_error: 1.1202 - val_loss: 0.5972 - val_root_mean_squared_error: 0.7509 - lr: 1.0000e-08\n",
      "Epoch 189/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.2900 - root_mean_squared_error: 1.1210\n",
      "Epoch 189: val_loss did not improve from 0.59292\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2900 - root_mean_squared_error: 1.1210 - val_loss: 0.5978 - val_root_mean_squared_error: 0.7513 - lr: 1.0000e-08\n",
      "Epoch 190/500\n",
      "10809/10811 [============================>.] - ETA: 0s - loss: 1.2931 - root_mean_squared_error: 1.1224\n",
      "Epoch 190: val_loss did not improve from 0.59292\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2931 - root_mean_squared_error: 1.1224 - val_loss: 0.6021 - val_root_mean_squared_error: 0.7541 - lr: 1.0000e-08\n",
      "Epoch 191/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.2925 - root_mean_squared_error: 1.1221\n",
      "Epoch 191: val_loss did not improve from 0.59292\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2925 - root_mean_squared_error: 1.1221 - val_loss: 0.6007 - val_root_mean_squared_error: 0.7532 - lr: 1.0000e-08\n",
      "Epoch 192/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.2880 - root_mean_squared_error: 1.1201\n",
      "Epoch 192: val_loss improved from 0.59292 to 0.59256, saving model to .mdl_wts/\n",
      "10811/10811 [==============================] - 86s 8ms/step - loss: 1.2883 - root_mean_squared_error: 1.1202 - val_loss: 0.5926 - val_root_mean_squared_error: 0.7478 - lr: 1.0000e-08\n",
      "Epoch 193/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.2919 - root_mean_squared_error: 1.1219\n",
      "Epoch 193: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2919 - root_mean_squared_error: 1.1218 - val_loss: 0.5979 - val_root_mean_squared_error: 0.7514 - lr: 1.0000e-08\n",
      "Epoch 194/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.2891 - root_mean_squared_error: 1.1206\n",
      "Epoch 194: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2890 - root_mean_squared_error: 1.1206 - val_loss: 0.6020 - val_root_mean_squared_error: 0.7541 - lr: 1.0000e-08\n",
      "Epoch 195/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.2941 - root_mean_squared_error: 1.1228\n",
      "Epoch 195: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2943 - root_mean_squared_error: 1.1229 - val_loss: 0.5982 - val_root_mean_squared_error: 0.7516 - lr: 1.0000e-08\n",
      "Epoch 196/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.2927 - root_mean_squared_error: 1.1222\n",
      "Epoch 196: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2929 - root_mean_squared_error: 1.1223 - val_loss: 0.5954 - val_root_mean_squared_error: 0.7497 - lr: 1.0000e-08\n",
      "Epoch 197/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.2921 - root_mean_squared_error: 1.1219\n",
      "Epoch 197: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2921 - root_mean_squared_error: 1.1219 - val_loss: 0.6014 - val_root_mean_squared_error: 0.7537 - lr: 1.0000e-08\n",
      "Epoch 198/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.2960 - root_mean_squared_error: 1.1237\n",
      "Epoch 198: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2962 - root_mean_squared_error: 1.1238 - val_loss: 0.6012 - val_root_mean_squared_error: 0.7536 - lr: 1.0000e-08\n",
      "Epoch 199/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 1.2928 - root_mean_squared_error: 1.1223\n",
      "Epoch 199: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2928 - root_mean_squared_error: 1.1223 - val_loss: 0.5943 - val_root_mean_squared_error: 0.7490 - lr: 1.0000e-08\n",
      "Epoch 200/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.2936 - root_mean_squared_error: 1.1226\n",
      "Epoch 200: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2936 - root_mean_squared_error: 1.1226 - val_loss: 0.5959 - val_root_mean_squared_error: 0.7500 - lr: 1.0000e-08\n",
      "Epoch 201/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.2939 - root_mean_squared_error: 1.1228\n",
      "Epoch 201: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2938 - root_mean_squared_error: 1.1227 - val_loss: 0.6003 - val_root_mean_squared_error: 0.7529 - lr: 1.0000e-08\n",
      "Epoch 202/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.2924 - root_mean_squared_error: 1.1221\n",
      "Epoch 202: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2925 - root_mean_squared_error: 1.1221 - val_loss: 0.5971 - val_root_mean_squared_error: 0.7509 - lr: 1.0000e-08\n",
      "Epoch 203/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.2930 - root_mean_squared_error: 1.1224\n",
      "Epoch 203: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2928 - root_mean_squared_error: 1.1223 - val_loss: 0.5971 - val_root_mean_squared_error: 0.7508 - lr: 1.0000e-08\n",
      "Epoch 204/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.2926 - root_mean_squared_error: 1.1222\n",
      "Epoch 204: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2925 - root_mean_squared_error: 1.1221 - val_loss: 0.6031 - val_root_mean_squared_error: 0.7548 - lr: 1.0000e-08\n",
      "Epoch 205/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.2927 - root_mean_squared_error: 1.1222\n",
      "Epoch 205: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2925 - root_mean_squared_error: 1.1221 - val_loss: 0.5975 - val_root_mean_squared_error: 0.7511 - lr: 1.0000e-08\n",
      "Epoch 206/500\n",
      "10809/10811 [============================>.] - ETA: 0s - loss: 1.2932 - root_mean_squared_error: 1.1225\n",
      "Epoch 206: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2932 - root_mean_squared_error: 1.1224 - val_loss: 0.5965 - val_root_mean_squared_error: 0.7505 - lr: 1.0000e-08\n",
      "Epoch 207/500\n",
      "10809/10811 [============================>.] - ETA: 0s - loss: 1.2916 - root_mean_squared_error: 1.1217\n",
      "Epoch 207: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2917 - root_mean_squared_error: 1.1218 - val_loss: 0.5961 - val_root_mean_squared_error: 0.7502 - lr: 1.0000e-08\n",
      "Epoch 208/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.2921 - root_mean_squared_error: 1.1220\n",
      "Epoch 208: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2922 - root_mean_squared_error: 1.1220 - val_loss: 0.6011 - val_root_mean_squared_error: 0.7535 - lr: 1.0000e-08\n",
      "Epoch 209/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.2913 - root_mean_squared_error: 1.1216\n",
      "Epoch 209: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2912 - root_mean_squared_error: 1.1216 - val_loss: 0.5947 - val_root_mean_squared_error: 0.7492 - lr: 1.0000e-08\n",
      "Epoch 210/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.2904 - root_mean_squared_error: 1.1212\n",
      "Epoch 210: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2903 - root_mean_squared_error: 1.1211 - val_loss: 0.6028 - val_root_mean_squared_error: 0.7547 - lr: 1.0000e-08\n",
      "Epoch 211/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.2901 - root_mean_squared_error: 1.1211\n",
      "Epoch 211: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2901 - root_mean_squared_error: 1.1210 - val_loss: 0.5960 - val_root_mean_squared_error: 0.7501 - lr: 1.0000e-08\n",
      "Epoch 212/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.2906 - root_mean_squared_error: 1.1213\n",
      "Epoch 212: val_loss did not improve from 0.59256\n",
      "\n",
      "Epoch 212: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2907 - root_mean_squared_error: 1.1213 - val_loss: 0.6029 - val_root_mean_squared_error: 0.7547 - lr: 1.0000e-08\n",
      "Epoch 213/500\n",
      "10809/10811 [============================>.] - ETA: 0s - loss: 1.2904 - root_mean_squared_error: 1.1212\n",
      "Epoch 213: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2904 - root_mean_squared_error: 1.1212 - val_loss: 0.6029 - val_root_mean_squared_error: 0.7547 - lr: 1.0000e-09\n",
      "Epoch 214/500\n",
      "10809/10811 [============================>.] - ETA: 0s - loss: 1.2912 - root_mean_squared_error: 1.1216\n",
      "Epoch 214: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2912 - root_mean_squared_error: 1.1215 - val_loss: 0.5938 - val_root_mean_squared_error: 0.7487 - lr: 1.0000e-09\n",
      "Epoch 215/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.2936 - root_mean_squared_error: 1.1226\n",
      "Epoch 215: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2936 - root_mean_squared_error: 1.1226 - val_loss: 0.5948 - val_root_mean_squared_error: 0.7493 - lr: 1.0000e-09\n",
      "Epoch 216/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.2916 - root_mean_squared_error: 1.1217\n",
      "Epoch 216: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2916 - root_mean_squared_error: 1.1217 - val_loss: 0.5989 - val_root_mean_squared_error: 0.7520 - lr: 1.0000e-09\n",
      "Epoch 217/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.2928 - root_mean_squared_error: 1.1223\n",
      "Epoch 217: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2928 - root_mean_squared_error: 1.1223 - val_loss: 0.6005 - val_root_mean_squared_error: 0.7531 - lr: 1.0000e-09\n",
      "Epoch 218/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.2934 - root_mean_squared_error: 1.1225\n",
      "Epoch 218: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2932 - root_mean_squared_error: 1.1224 - val_loss: 0.5986 - val_root_mean_squared_error: 0.7518 - lr: 1.0000e-09\n",
      "Epoch 219/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.2906 - root_mean_squared_error: 1.1213\n",
      "Epoch 219: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2909 - root_mean_squared_error: 1.1214 - val_loss: 0.5975 - val_root_mean_squared_error: 0.7511 - lr: 1.0000e-09\n",
      "Epoch 220/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.2907 - root_mean_squared_error: 1.1213\n",
      "Epoch 220: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2907 - root_mean_squared_error: 1.1213 - val_loss: 0.5989 - val_root_mean_squared_error: 0.7521 - lr: 1.0000e-09\n",
      "Epoch 221/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.2909 - root_mean_squared_error: 1.1214\n",
      "Epoch 221: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2908 - root_mean_squared_error: 1.1214 - val_loss: 0.6000 - val_root_mean_squared_error: 0.7528 - lr: 1.0000e-09\n",
      "Epoch 222/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.2915 - root_mean_squared_error: 1.1217\n",
      "Epoch 222: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2915 - root_mean_squared_error: 1.1217 - val_loss: 0.5969 - val_root_mean_squared_error: 0.7507 - lr: 1.0000e-09\n",
      "Epoch 223/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 1.2934 - root_mean_squared_error: 1.1225\n",
      "Epoch 223: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2934 - root_mean_squared_error: 1.1225 - val_loss: 0.5971 - val_root_mean_squared_error: 0.7508 - lr: 1.0000e-09\n",
      "Epoch 224/500\n",
      "10809/10811 [============================>.] - ETA: 0s - loss: 1.2904 - root_mean_squared_error: 1.1212\n",
      "Epoch 224: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2906 - root_mean_squared_error: 1.1213 - val_loss: 0.5944 - val_root_mean_squared_error: 0.7490 - lr: 1.0000e-09\n",
      "Epoch 225/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.2919 - root_mean_squared_error: 1.1219\n",
      "Epoch 225: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2917 - root_mean_squared_error: 1.1218 - val_loss: 0.5961 - val_root_mean_squared_error: 0.7502 - lr: 1.0000e-09\n",
      "Epoch 226/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 1.2893 - root_mean_squared_error: 1.1207\n",
      "Epoch 226: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2893 - root_mean_squared_error: 1.1207 - val_loss: 0.5945 - val_root_mean_squared_error: 0.7491 - lr: 1.0000e-09\n",
      "Epoch 227/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.2939 - root_mean_squared_error: 1.1228\n",
      "Epoch 227: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2939 - root_mean_squared_error: 1.1227 - val_loss: 0.5970 - val_root_mean_squared_error: 0.7508 - lr: 1.0000e-09\n",
      "Epoch 228/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.2920 - root_mean_squared_error: 1.1219\n",
      "Epoch 228: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2918 - root_mean_squared_error: 1.1218 - val_loss: 0.5982 - val_root_mean_squared_error: 0.7516 - lr: 1.0000e-09\n",
      "Epoch 229/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.2890 - root_mean_squared_error: 1.1206\n",
      "Epoch 229: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2889 - root_mean_squared_error: 1.1205 - val_loss: 0.6001 - val_root_mean_squared_error: 0.7529 - lr: 1.0000e-09\n",
      "Epoch 230/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 1.2926 - root_mean_squared_error: 1.1221\n",
      "Epoch 230: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2926 - root_mean_squared_error: 1.1221 - val_loss: 0.5993 - val_root_mean_squared_error: 0.7523 - lr: 1.0000e-09\n",
      "Epoch 231/500\n",
      "10809/10811 [============================>.] - ETA: 0s - loss: 1.2923 - root_mean_squared_error: 1.1220\n",
      "Epoch 231: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2925 - root_mean_squared_error: 1.1221 - val_loss: 0.6006 - val_root_mean_squared_error: 0.7531 - lr: 1.0000e-09\n",
      "Epoch 232/500\n",
      "10809/10811 [============================>.] - ETA: 0s - loss: 1.2916 - root_mean_squared_error: 1.1217\n",
      "Epoch 232: val_loss did not improve from 0.59256\n",
      "\n",
      "Epoch 232: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2915 - root_mean_squared_error: 1.1217 - val_loss: 0.5972 - val_root_mean_squared_error: 0.7509 - lr: 1.0000e-09\n",
      "Epoch 233/500\n",
      "10809/10811 [============================>.] - ETA: 0s - loss: 1.2911 - root_mean_squared_error: 1.1215\n",
      "Epoch 233: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2910 - root_mean_squared_error: 1.1215 - val_loss: 0.5958 - val_root_mean_squared_error: 0.7500 - lr: 1.0000e-10\n",
      "Epoch 234/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.2896 - root_mean_squared_error: 1.1208\n",
      "Epoch 234: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2896 - root_mean_squared_error: 1.1208 - val_loss: 0.5976 - val_root_mean_squared_error: 0.7511 - lr: 1.0000e-10\n",
      "Epoch 235/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.2923 - root_mean_squared_error: 1.1220\n",
      "Epoch 235: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2923 - root_mean_squared_error: 1.1220 - val_loss: 0.5964 - val_root_mean_squared_error: 0.7504 - lr: 1.0000e-10\n",
      "Epoch 236/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.2918 - root_mean_squared_error: 1.1218\n",
      "Epoch 236: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2918 - root_mean_squared_error: 1.1218 - val_loss: 0.6016 - val_root_mean_squared_error: 0.7538 - lr: 1.0000e-10\n",
      "Epoch 237/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.2901 - root_mean_squared_error: 1.1211\n",
      "Epoch 237: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2901 - root_mean_squared_error: 1.1211 - val_loss: 0.5947 - val_root_mean_squared_error: 0.7493 - lr: 1.0000e-10\n",
      "Epoch 238/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.2880 - root_mean_squared_error: 1.1201\n",
      "Epoch 238: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2893 - root_mean_squared_error: 1.1207 - val_loss: 0.5981 - val_root_mean_squared_error: 0.7515 - lr: 1.0000e-10\n",
      "Epoch 239/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.2895 - root_mean_squared_error: 1.1208\n",
      "Epoch 239: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2895 - root_mean_squared_error: 1.1208 - val_loss: 0.5980 - val_root_mean_squared_error: 0.7514 - lr: 1.0000e-10\n",
      "Epoch 240/500\n",
      "10809/10811 [============================>.] - ETA: 0s - loss: 1.2899 - root_mean_squared_error: 1.1210\n",
      "Epoch 240: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2900 - root_mean_squared_error: 1.1210 - val_loss: 0.5975 - val_root_mean_squared_error: 0.7511 - lr: 1.0000e-10\n",
      "Epoch 241/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.2912 - root_mean_squared_error: 1.1215\n",
      "Epoch 241: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2917 - root_mean_squared_error: 1.1218 - val_loss: 0.5958 - val_root_mean_squared_error: 0.7499 - lr: 1.0000e-10\n",
      "Epoch 242/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.2891 - root_mean_squared_error: 1.1206\n",
      "Epoch 242: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2890 - root_mean_squared_error: 1.1206 - val_loss: 0.5968 - val_root_mean_squared_error: 0.7507 - lr: 1.0000e-10\n",
      "Epoch 243/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 1.2932 - root_mean_squared_error: 1.1224\n",
      "Epoch 243: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2932 - root_mean_squared_error: 1.1224 - val_loss: 0.5956 - val_root_mean_squared_error: 0.7499 - lr: 1.0000e-10\n",
      "Epoch 244/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.2922 - root_mean_squared_error: 1.1220\n",
      "Epoch 244: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2922 - root_mean_squared_error: 1.1220 - val_loss: 0.5973 - val_root_mean_squared_error: 0.7510 - lr: 1.0000e-10\n",
      "Epoch 245/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.2934 - root_mean_squared_error: 1.1225\n",
      "Epoch 245: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 86s 8ms/step - loss: 1.2933 - root_mean_squared_error: 1.1225 - val_loss: 0.6017 - val_root_mean_squared_error: 0.7539 - lr: 1.0000e-10\n",
      "Epoch 246/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.2896 - root_mean_squared_error: 1.1208\n",
      "Epoch 246: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2896 - root_mean_squared_error: 1.1208 - val_loss: 0.5989 - val_root_mean_squared_error: 0.7520 - lr: 1.0000e-10\n",
      "Epoch 247/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.2917 - root_mean_squared_error: 1.1217\n",
      "Epoch 247: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2917 - root_mean_squared_error: 1.1218 - val_loss: 0.5983 - val_root_mean_squared_error: 0.7517 - lr: 1.0000e-10\n",
      "Epoch 248/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.2933 - root_mean_squared_error: 1.1225\n",
      "Epoch 248: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2932 - root_mean_squared_error: 1.1225 - val_loss: 0.5955 - val_root_mean_squared_error: 0.7497 - lr: 1.0000e-10\n",
      "Epoch 249/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.2917 - root_mean_squared_error: 1.1218\n",
      "Epoch 249: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2917 - root_mean_squared_error: 1.1218 - val_loss: 0.5962 - val_root_mean_squared_error: 0.7502 - lr: 1.0000e-10\n",
      "Epoch 250/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.2919 - root_mean_squared_error: 1.1218\n",
      "Epoch 250: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2918 - root_mean_squared_error: 1.1218 - val_loss: 0.5966 - val_root_mean_squared_error: 0.7505 - lr: 1.0000e-10\n",
      "Epoch 251/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 1.2898 - root_mean_squared_error: 1.1209\n",
      "Epoch 251: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2898 - root_mean_squared_error: 1.1209 - val_loss: 0.5961 - val_root_mean_squared_error: 0.7501 - lr: 1.0000e-10\n",
      "Epoch 252/500\n",
      "10809/10811 [============================>.] - ETA: 0s - loss: 1.2918 - root_mean_squared_error: 1.1218\n",
      "Epoch 252: val_loss did not improve from 0.59256\n",
      "\n",
      "Epoch 252: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2918 - root_mean_squared_error: 1.1218 - val_loss: 0.5993 - val_root_mean_squared_error: 0.7523 - lr: 1.0000e-10\n",
      "Epoch 253/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.2885 - root_mean_squared_error: 1.1203\n",
      "Epoch 253: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2886 - root_mean_squared_error: 1.1204 - val_loss: 0.6003 - val_root_mean_squared_error: 0.7530 - lr: 1.0000e-11\n",
      "Epoch 254/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.2893 - root_mean_squared_error: 1.1207\n",
      "Epoch 254: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2896 - root_mean_squared_error: 1.1208 - val_loss: 0.5960 - val_root_mean_squared_error: 0.7501 - lr: 1.0000e-11\n",
      "Epoch 255/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.2913 - root_mean_squared_error: 1.1216\n",
      "Epoch 255: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2913 - root_mean_squared_error: 1.1216 - val_loss: 0.5959 - val_root_mean_squared_error: 0.7500 - lr: 1.0000e-11\n",
      "Epoch 256/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.2929 - root_mean_squared_error: 1.1223\n",
      "Epoch 256: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2929 - root_mean_squared_error: 1.1223 - val_loss: 0.5973 - val_root_mean_squared_error: 0.7510 - lr: 1.0000e-11\n",
      "Epoch 257/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.2937 - root_mean_squared_error: 1.1226\n",
      "Epoch 257: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2937 - root_mean_squared_error: 1.1226 - val_loss: 0.5980 - val_root_mean_squared_error: 0.7515 - lr: 1.0000e-11\n",
      "Epoch 258/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.2912 - root_mean_squared_error: 1.1215\n",
      "Epoch 258: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2910 - root_mean_squared_error: 1.1214 - val_loss: 0.5979 - val_root_mean_squared_error: 0.7513 - lr: 1.0000e-11\n",
      "Epoch 259/500\n",
      "10809/10811 [============================>.] - ETA: 0s - loss: 1.2951 - root_mean_squared_error: 1.1233\n",
      "Epoch 259: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2950 - root_mean_squared_error: 1.1232 - val_loss: 0.5966 - val_root_mean_squared_error: 0.7505 - lr: 1.0000e-11\n",
      "Epoch 260/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.2930 - root_mean_squared_error: 1.1223\n",
      "Epoch 260: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2929 - root_mean_squared_error: 1.1223 - val_loss: 0.5946 - val_root_mean_squared_error: 0.7492 - lr: 1.0000e-11\n",
      "Epoch 261/500\n",
      "10809/10811 [============================>.] - ETA: 0s - loss: 1.2909 - root_mean_squared_error: 1.1214\n",
      "Epoch 261: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2913 - root_mean_squared_error: 1.1216 - val_loss: 0.5991 - val_root_mean_squared_error: 0.7521 - lr: 1.0000e-11\n",
      "Epoch 262/500\n",
      "10809/10811 [============================>.] - ETA: 0s - loss: 1.2915 - root_mean_squared_error: 1.1217\n",
      "Epoch 262: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2916 - root_mean_squared_error: 1.1217 - val_loss: 0.5967 - val_root_mean_squared_error: 0.7506 - lr: 1.0000e-11\n",
      "Epoch 263/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 1.2902 - root_mean_squared_error: 1.1211\n",
      "Epoch 263: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2902 - root_mean_squared_error: 1.1211 - val_loss: 0.5959 - val_root_mean_squared_error: 0.7500 - lr: 1.0000e-11\n",
      "Epoch 264/500\n",
      "10809/10811 [============================>.] - ETA: 0s - loss: 1.2923 - root_mean_squared_error: 1.1220\n",
      "Epoch 264: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2923 - root_mean_squared_error: 1.1220 - val_loss: 0.5970 - val_root_mean_squared_error: 0.7507 - lr: 1.0000e-11\n",
      "Epoch 265/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.2917 - root_mean_squared_error: 1.1217\n",
      "Epoch 265: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2917 - root_mean_squared_error: 1.1218 - val_loss: 0.6001 - val_root_mean_squared_error: 0.7528 - lr: 1.0000e-11\n",
      "Epoch 266/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.2915 - root_mean_squared_error: 1.1217\n",
      "Epoch 266: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2914 - root_mean_squared_error: 1.1216 - val_loss: 0.5977 - val_root_mean_squared_error: 0.7513 - lr: 1.0000e-11\n",
      "Epoch 267/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.2914 - root_mean_squared_error: 1.1216\n",
      "Epoch 267: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2914 - root_mean_squared_error: 1.1216 - val_loss: 0.5983 - val_root_mean_squared_error: 0.7517 - lr: 1.0000e-11\n",
      "Epoch 268/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.2915 - root_mean_squared_error: 1.1217\n",
      "Epoch 268: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2914 - root_mean_squared_error: 1.1216 - val_loss: 0.6005 - val_root_mean_squared_error: 0.7531 - lr: 1.0000e-11\n",
      "Epoch 269/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 1.2954 - root_mean_squared_error: 1.1234\n",
      "Epoch 269: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2954 - root_mean_squared_error: 1.1234 - val_loss: 0.5997 - val_root_mean_squared_error: 0.7526 - lr: 1.0000e-11\n",
      "Epoch 270/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.2931 - root_mean_squared_error: 1.1224\n",
      "Epoch 270: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2930 - root_mean_squared_error: 1.1223 - val_loss: 0.5999 - val_root_mean_squared_error: 0.7527 - lr: 1.0000e-11\n",
      "Epoch 271/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 1.2893 - root_mean_squared_error: 1.1207\n",
      "Epoch 271: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2893 - root_mean_squared_error: 1.1207 - val_loss: 0.5973 - val_root_mean_squared_error: 0.7510 - lr: 1.0000e-11\n",
      "Epoch 272/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.2915 - root_mean_squared_error: 1.1217\n",
      "Epoch 272: val_loss did not improve from 0.59256\n",
      "\n",
      "Epoch 272: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-12.\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2915 - root_mean_squared_error: 1.1217 - val_loss: 0.5957 - val_root_mean_squared_error: 0.7499 - lr: 1.0000e-11\n",
      "Epoch 273/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 1.2938 - root_mean_squared_error: 1.1227\n",
      "Epoch 273: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2938 - root_mean_squared_error: 1.1227 - val_loss: 0.5953 - val_root_mean_squared_error: 0.7496 - lr: 1.0000e-12\n",
      "Epoch 274/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.2939 - root_mean_squared_error: 1.1228\n",
      "Epoch 274: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2939 - root_mean_squared_error: 1.1228 - val_loss: 0.5996 - val_root_mean_squared_error: 0.7525 - lr: 1.0000e-12\n",
      "Epoch 275/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.2903 - root_mean_squared_error: 1.1212\n",
      "Epoch 275: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2903 - root_mean_squared_error: 1.1211 - val_loss: 0.5950 - val_root_mean_squared_error: 0.7494 - lr: 1.0000e-12\n",
      "Epoch 276/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.2919 - root_mean_squared_error: 1.1219\n",
      "Epoch 276: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2918 - root_mean_squared_error: 1.1218 - val_loss: 0.5930 - val_root_mean_squared_error: 0.7481 - lr: 1.0000e-12\n",
      "Epoch 277/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.2907 - root_mean_squared_error: 1.1213\n",
      "Epoch 277: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2906 - root_mean_squared_error: 1.1213 - val_loss: 0.5995 - val_root_mean_squared_error: 0.7524 - lr: 1.0000e-12\n",
      "Epoch 278/500\n",
      "10809/10811 [============================>.] - ETA: 0s - loss: 1.2903 - root_mean_squared_error: 1.1211\n",
      "Epoch 278: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2903 - root_mean_squared_error: 1.1211 - val_loss: 0.5969 - val_root_mean_squared_error: 0.7507 - lr: 1.0000e-12\n",
      "Epoch 279/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.2928 - root_mean_squared_error: 1.1223\n",
      "Epoch 279: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2927 - root_mean_squared_error: 1.1222 - val_loss: 0.5997 - val_root_mean_squared_error: 0.7526 - lr: 1.0000e-12\n",
      "Epoch 280/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.2901 - root_mean_squared_error: 1.1210\n",
      "Epoch 280: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2900 - root_mean_squared_error: 1.1210 - val_loss: 0.6016 - val_root_mean_squared_error: 0.7539 - lr: 1.0000e-12\n",
      "Epoch 281/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.2902 - root_mean_squared_error: 1.1211\n",
      "Epoch 281: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2904 - root_mean_squared_error: 1.1212 - val_loss: 0.5973 - val_root_mean_squared_error: 0.7510 - lr: 1.0000e-12\n",
      "Epoch 282/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.2917 - root_mean_squared_error: 1.1217\n",
      "Epoch 282: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2917 - root_mean_squared_error: 1.1217 - val_loss: 0.5978 - val_root_mean_squared_error: 0.7513 - lr: 1.0000e-12\n",
      "Epoch 283/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.2944 - root_mean_squared_error: 1.1230\n",
      "Epoch 283: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2944 - root_mean_squared_error: 1.1229 - val_loss: 0.5932 - val_root_mean_squared_error: 0.7482 - lr: 1.0000e-12\n",
      "Epoch 284/500\n",
      "10808/10811 [============================>.] - ETA: 0s - loss: 1.2933 - root_mean_squared_error: 1.1225\n",
      "Epoch 284: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2933 - root_mean_squared_error: 1.1225 - val_loss: 0.5967 - val_root_mean_squared_error: 0.7506 - lr: 1.0000e-12\n",
      "Epoch 285/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.2948 - root_mean_squared_error: 1.1231\n",
      "Epoch 285: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2949 - root_mean_squared_error: 1.1232 - val_loss: 0.5945 - val_root_mean_squared_error: 0.7491 - lr: 1.0000e-12\n",
      "Epoch 286/500\n",
      "10810/10811 [============================>.] - ETA: 0s - loss: 1.2930 - root_mean_squared_error: 1.1224\n",
      "Epoch 286: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2930 - root_mean_squared_error: 1.1224 - val_loss: 0.5968 - val_root_mean_squared_error: 0.7507 - lr: 1.0000e-12\n",
      "Epoch 287/500\n",
      "10807/10811 [============================>.] - ETA: 0s - loss: 1.2916 - root_mean_squared_error: 1.1217\n",
      "Epoch 287: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2916 - root_mean_squared_error: 1.1217 - val_loss: 0.6024 - val_root_mean_squared_error: 0.7544 - lr: 1.0000e-12\n",
      "Epoch 288/500\n",
      "10805/10811 [============================>.] - ETA: 0s - loss: 1.2966 - root_mean_squared_error: 1.1240\n",
      "Epoch 288: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2965 - root_mean_squared_error: 1.1239 - val_loss: 0.5966 - val_root_mean_squared_error: 0.7505 - lr: 1.0000e-12\n",
      "Epoch 289/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.2948 - root_mean_squared_error: 1.1232\n",
      "Epoch 289: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2948 - root_mean_squared_error: 1.1231 - val_loss: 0.5980 - val_root_mean_squared_error: 0.7514 - lr: 1.0000e-12\n",
      "Epoch 290/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 1.2958 - root_mean_squared_error: 1.1236\n",
      "Epoch 290: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2958 - root_mean_squared_error: 1.1236 - val_loss: 0.5968 - val_root_mean_squared_error: 0.7506 - lr: 1.0000e-12\n",
      "Epoch 291/500\n",
      "10806/10811 [============================>.] - ETA: 0s - loss: 1.2923 - root_mean_squared_error: 1.1220\n",
      "Epoch 291: val_loss did not improve from 0.59256\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2921 - root_mean_squared_error: 1.1219 - val_loss: 0.5982 - val_root_mean_squared_error: 0.7516 - lr: 1.0000e-12\n",
      "Epoch 292/500\n",
      "10811/10811 [==============================] - ETA: 0s - loss: 1.2899 - root_mean_squared_error: 1.1209\n",
      "Epoch 292: val_loss did not improve from 0.59256\n",
      "\n",
      "Epoch 292: ReduceLROnPlateau reducing learning rate to 1.0000001044244145e-13.\n",
      "10811/10811 [==============================] - 85s 8ms/step - loss: 1.2899 - root_mean_squared_error: 1.1209 - val_loss: 0.5965 - val_root_mean_squared_error: 0.7505 - lr: 1.0000e-12\n",
      "Epoch 292: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train the regressor\n",
    "\n",
    "# format input data as tensors\n",
    "X = np.array(list(feature_matrix_train.keys())).reshape(-1,175)\n",
    "y = np.array(list(feature_matrix_train.values())).reshape(-1,1) #/ OUTPUT_SCALE\n",
    "\n",
    "mcp_save = ModelCheckpoint(checkpoint_filepath, save_best_only=True, save_weights_only=True,\n",
    "                          monitor='val_loss', mode='min', verbose=1)\n",
    "\n",
    "early_stop = EarlyStopping(patience=PATIENCE, restore_best_weights=False,\n",
    "                          monitor='val_loss', mode='min', verbose=1)\n",
    "\n",
    "reduce_lr_loss = ReduceLROnPlateau(factor=LR_PLATEAU_FACTOR, patience=LR_PLATEAU_PATIENCE, min_delta=1e-6,\n",
    "                          monitor='val_loss', mode='min', verbose=1)\n",
    "\n",
    "history = regressor.fit([X[:,0],  X[:,1],  X[:,2],  \n",
    "                         X[:,3:8].reshape((-1, 1, 5)),  X[:,8:].reshape((-1, 1, 167))], y,\n",
    "                        validation_split=VALIDATION_SPLIT,\n",
    "                        callbacks=[mcp_save, early_stop, reduce_lr_loss],\n",
    "                        epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T13:56:59.813923Z",
     "iopub.status.busy": "2023-11-28T13:56:59.813621Z",
     "iopub.status.idle": "2023-11-28T13:57:00.917531Z",
     "shell.execute_reply": "2023-11-28T13:57:00.916950Z",
     "shell.execute_reply.started": "2023-11-28T13:56:59.813906Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 516\n",
      "drwxrwsr-x  6 jovyan jovyan   4096 Nov 28 13:56 .\n",
      "drwxr-Sr-- 19 jovyan jovyan   4096 Oct 27 23:43 ..\n",
      "-rw-rw-r--  1 jovyan jovyan 239497 Oct 29 23:00 03_model_training.ipynb\n",
      "-rw-rw-r--  1 jovyan jovyan 184394 Nov 28 13:56 04_model_training.ipynb\n",
      "-rw-rw-r--  1 jovyan jovyan  65108 Nov 28 07:26 05_prediction.ipynb\n",
      "-rw-rw-r--  1 jovyan jovyan     73 Oct 27 23:25 checkpoint\n",
      "drwxrwsr-x  2 jovyan jovyan   4096 Oct 30 02:38 .ipynb_checkpoints\n",
      "drwxr-sr-x  2 jovyan jovyan   4096 Nov 28 11:34 .mdl_wts\n",
      "-rw-rw-r--  1 jovyan jovyan   2501 Oct 30 02:37 name_to_id_maps_for_embeddings.txt\n",
      "drwxrwsr-x  3 jovyan jovyan   4096 Oct 27 16:37 neurips-2023-scripts\n",
      "drwxrwsr-x  6 jovyan jovyan   4096 Oct 28 23:13 scp\n",
      "total 112064\n",
      "drwxr-sr-x 2 jovyan jovyan      4096 Nov 28 11:34 .\n",
      "drwxrwsr-x 6 jovyan jovyan      4096 Nov 28 13:56 ..\n",
      "-rw-r--r-- 1 jovyan jovyan        59 Nov 28 11:34 checkpoint\n",
      "-rw-r--r-- 1 jovyan jovyan 114731558 Nov 28 11:34 .data-00000-of-00001\n",
      "-rw-r--r-- 1 jovyan jovyan      6065 Nov 28 11:34 .index\n"
     ]
    }
   ],
   "source": [
    "! ls -la \n",
    "! ls -la .mdl_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T13:57:00.918523Z",
     "iopub.status.busy": "2023-11-28T13:57:00.918334Z",
     "iopub.status.idle": "2023-11-28T13:57:01.046638Z",
     "shell.execute_reply": "2023-11-28T13:57:01.046063Z",
     "shell.execute_reply.started": "2023-11-28T13:57:00.918505Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fd6740f5a30>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The model weights (that are considered the best) are loaded into the\n",
    "# model from the temporary checkpoint path in the instance.\n",
    "regressor.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T13:57:01.048238Z",
     "iopub.status.busy": "2023-11-28T13:57:01.047789Z",
     "iopub.status.idle": "2023-11-28T13:57:01.209454Z",
     "shell.execute_reply": "2023-11-28T13:57:01.208868Z",
     "shell.execute_reply.started": "2023-11-28T13:57:01.048217Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./scp/model/20231128_1357.keras\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "# Get today's date in the format YYYYMMDDHHMM\n",
    "formatted_datetime = datetime.datetime.now().strftime('%Y%m%d_%H%M')\n",
    "# save model in permanent storage\n",
    "file_path = f\"{model_path}/{formatted_datetime}.keras\"\n",
    "regressor.save(file_path)\n",
    "print(f'Model saved to {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 927
    },
    "execution": {
     "iopub.execute_input": "2023-11-28T13:57:01.210316Z",
     "iopub.status.busy": "2023-11-28T13:57:01.210148Z",
     "iopub.status.idle": "2023-11-28T13:57:01.955710Z",
     "shell.execute_reply": "2023-11-28T13:57:01.955143Z",
     "shell.execute_reply.started": "2023-11-28T13:57:01.210302Z"
    },
    "id": "u_9aDn8NSCub",
    "outputId": "dde86813-95e5-4b17-bb85-ee2491f9f063",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "#plot the training and validation accuracy and loss at each epoch\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(f\"{model_path}/{formatted_datetime}_loss.png\")\n",
    "plt.show() # show image, create new image and set it as current\n",
    "\n",
    "acc = history.history['root_mean_squared_error']\n",
    "val_acc = history.history['val_root_mean_squared_error']\n",
    "plt.plot(epochs, acc, 'y', label='Training RMSE')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation RMSE')\n",
    "plt.title('Training and validation RMSE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(f\"{model_path}/{formatted_datetime}_rmse.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T13:57:01.956791Z",
     "iopub.status.busy": "2023-11-28T13:57:01.956488Z",
     "iopub.status.idle": "2023-11-28T13:57:03.739981Z",
     "shell.execute_reply": "2023-11-28T13:57:03.739407Z",
     "shell.execute_reply.started": "2023-11-28T13:57:01.956774Z"
    },
    "id": "Y_lScQnEhRYN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T13:57:03.741389Z",
     "iopub.status.busy": "2023-11-28T13:57:03.740712Z",
     "iopub.status.idle": "2023-11-28T13:57:03.743899Z",
     "shell.execute_reply": "2023-11-28T13:57:03.743420Z",
     "shell.execute_reply.started": "2023-11-28T13:57:03.741371Z"
    },
    "id": "tC_9HyL4g2hn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 10-fold Cross Validation\n",
    "# kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=60)\n",
    "\n",
    "# scores = cross_val_score(\n",
    "#     estimator = regressor,\n",
    "#     X = feature_matrix_X,\n",
    "#     y = label_Y,\n",
    "#     scoring='mse',\n",
    "#     cv = kfold,\n",
    "#     n_jobs = 1)\n",
    "# scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T13:57:03.744844Z",
     "iopub.status.busy": "2023-11-28T13:57:03.744522Z",
     "iopub.status.idle": "2023-11-28T13:57:03.748269Z",
     "shell.execute_reply": "2023-11-28T13:57:03.747826Z",
     "shell.execute_reply.started": "2023-11-28T13:57:03.744823Z"
    },
    "id": "yvXabwHP7lOA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Evaluate the regressor\n",
    "# Y_preds = regressor.predict([X[:,0],  X[:,2],  X[:,2]], batch_size=2024)\n",
    "# print(f'Y_preds.shape: {Y_preds.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
